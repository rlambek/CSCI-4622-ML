{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Stochastic Gradient Descent and Logistic Regression\n",
    "***\n",
    "\n",
    "**Name**: \n",
    "\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Tuesday March 6th**. Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI-4622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy).\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle, gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 1 - MLE and SGD for the Exponential Distribution Rate Parameter\n",
    "***\n",
    "\n",
    "Suppose you're given $n$ numbers $x_1, x_2, \\ldots, x_n$ (think training data) and told that they're samples from the exponential distribution $Exp(\\lambda)$ where the rate parameter $\\lambda$ is unknown. Recall that the probability density function for $Exp(\\lambda)$ is given by  \n",
    "\n",
    "$$\n",
    "f_\\lambda(x) = \\left\\{\n",
    "\\begin{array}{rl}\n",
    "0 & \\textrm{if } x < 0 \\\\\n",
    "\\lambda e^{-\\lambda x} & \\textrm{if } x \\geq 0\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "In this problem we'll use Maximum Likelihood Estimation to estimate the rate parameter by hand and with Stochastic Gradient Descent. \n",
    "\n",
    "**Part A**: Write down the likelihood function $L(\\lambda)$ for the data set $x_1, x_2, \\ldots, x_n$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Write down the associated Negative Log-Likelihood $\\textrm{NLL}(\\lambda)$ and simplify it algebraically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Find a formula for the MLE of the rate parameter $\\lambda$ by taking the derivative of $\\textrm{NLL}(\\lambda)$, setting it equal to zero, and solving for $\\hat{\\lambda}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Use the formula you found in **Part C** to estimate the rate parameter $\\lambda$ for the following training data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lam = 3.0; x_train = np.random.exponential(1/lam, size=10) # Note: numpy's exponential sampler expects 1 over rate parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamhat = 0.0 # TODO \n",
    "print(\"MLE for Lambda = {:.3f}\".format(lamhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: Describe a **Stochastic** Gradient Descent algorithm based on the $\\textrm{NLL}$ you found in **Part B**. **Hint**: Think of what the loss function would be if the training set contained just a single point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part F**: Implement the scheme described in **Part E** and run it on your training set.  Does it converge to the MLE that you found in **Part D**? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Problem 2 - Regularized Logistic Regression Intuition \n",
    "***\n",
    "\n",
    "Consider the training set shown below where red dots correspond to training examples with label $y=1$ and blue dots correspond to training examples with label $y = 0$. Suppose you fit a logistic regression model of the form \n",
    "\n",
    "$$\n",
    "p(y = 1 \\mid {\\bf x}) = \\textrm{sigm}(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2) = \\dfrac{1}{1 + \\exp(-\\boldsymbol{\\beta}^T{\\bf x})}\n",
    "$$\n",
    "\n",
    "where here in $\\boldsymbol{\\beta}^T{\\bf x}$ the vector ${\\bf x}$ has had a $1$ prepended so that it looks like ${\\bf x} = (1, x_1, x_2)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUkAAAFGCAYAAADn3G19AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4nPV97v/3Z0Yz2ixb3o1lG9t4BQOWMauBGttsgaQh\nIRfkJGnTNDgn20nS3y9p09MSktPTNCcE6GlIiBvahkMIJ2FJCAHKZiBsBhsbvGO8G++yZe2a7XP+\nkGSEkR5tM/OM7ft1XbpsaZbnnpF06/ts38fcHRER6Vok7AAiIoVMJSkiEkAlKSISQCUpIhJAJSki\nEkAlKSISIJSSNLNvmNlaM1tjZr8ys5IwcoiI9CTvJWlmVcB/A+a6+ywgCtyY7xwiIr0R1up2EVBq\nZkVAGbA7pBwiIoGK8r1Ad3/XzG4FdgDNwJPu/mTn+5jZYmAxQFlZ2TnTpk3Ld8wupdNpotFo2DEA\nZemOsnStULIUSg6AVatWHXT3kT3dz/J9WqKZDQUeBG4AaoHfAA+4+71d3b+6utpXrlyZx4Tdq62t\npbKyMuwYgLJ0R1m6VihZCiUHgJmtcPe5Pd0vjNXtRcBWdz/g7kngIeCiEHKIiPQojJLcAVxgZmVm\nZsBCYH0IOUREepT3knT3ZcADwBvA6vYMS/KdQ0SkN/K+4wbA3b8DfCeMZYucLJLJJLt27aKlpYVM\nJsOePXvCjhRKjpKSEsaNG0csFuvX40MpSRHJvV27dlFRUcHEiRNJp9MUFYX/655KpfKaw92pqalh\n165dTJo0qV/PodMSRU5QLS0tDB8+nLZN/ycnM2P48OG0tLT0+zlUkiInsJO5IDsM9D1QSYqIBFBJ\nikjB+u1vf8u6deu6vO2WW26hqqqK2bNnM2PGDL74xS+SyWSynkElKSIFKZVKBZYkwDe+8Q1WrVrF\nunXrWL16Nc8//3zWc6gkRSQntm3bxowZM/jUpz7FzJkzuf7662lqagLge9/7Hueeey6zZs1i8eLF\ndJwePX/+fL7+9a8zd+5cfvCDH/DII4/wzW9+k9mzZ7N58+Zul5VIJGhpaWHo0KFZfx3hHxMgIjk3\n5e/+M2fPve2frun2to0bN3L33Xczb948Pve5z3HXXXfxrW99i6985SvcfPPNAHzmM5/h0Ucf5cMf\n/jDQVnjLly8HYNOmTVx77bVcf/31XT7/7bffzr333sv27du5+uqrmT17dpZfnUaSIpJD48ePZ968\neQB8+tOf5qWXXgJg6dKlnH/++Zx55pk8++yzrF279uhjbrjhhl4/f8fq9v79+2lsbOT+++/P7gtA\nJSkiOXTs4TdmRktLC1/60pd44IEHWL16NTfddNP7jmMsLy/v83JisRhXXXUVL7zwwoAzH0ur2yIn\ngXf+4cpQzrjZsWMHr7zyChdeeCH33Xcf8+bNO1qII0aMoKGhgQceeKDb1emKigrq6+t7XI6789JL\nL1FdXZ3V/KCRpIjk0PTp07nzzjuZOXMmhw8f5gtf+AKVlZXcdNNNzJo1iyuvvJJzzz2328ffeOON\n/PCHP6S6urrLHTe33347s2fPZtasWaTTab70pS9l/TXkfdLdvtKku11Tlq4py3vWr1/PzJkzgfyf\nMw1te7evvfZa1qxZc/RrYeSA978XHQp50l0RkeOGSlJEcmLixInvG0Uer1SSIiIBVJIiIgFUkiIi\nAVSSIiIBVJIiUrB6mgXonnvuYdasWZx55plUV1dz6623Zj2DSlJEClJPU6U9/vjj3HHHHTz55JOs\nXr2aV199lSFDhmQ9h0pSRHIi11Olff/73+fWW29l7NixABQXF3PTTTdl/XXo3G2Rk0DRPwzP3ZPf\ncqTbm3I5VdqaNWs455xzcvCC3k8jSRHJmVxPlZYPKkkRyZlcTpV2xhlnsGLFiqzm7UreV7fNbDrw\nfzt9aTJws7vfke8sIieL1N/VnHBTpX3729/mm9/8Jn/4wx8YM2YMiUSCe+65h89//vNZfQ15H0m6\n+0Z3n+3us4FzgCbg4XznEJHcy+VUaR/60If4yle+wqJFizjjjDOYM2cOdXV1WX8NoU6VZmZXAN9x\n93nd3UdTpXVNWbqmLO/RVGnvGchUaWHv3b4R+NWxXzSzxcBigKqqKmpra/Odq0u9mSE5X5Sla8ry\nnkwmQyqV+sD/86VjeZ2XG0aOjuX2t0dCK0kziwMfAb597G3uvgRYAm0jyUIZGQAFM0oBZemOsrTZ\ns2fP0VFbGCO4KVOmfGCqtLBGkpFIpN/fizD3bl8NvOHu+0LMICISKMyS/CRdrGqLiBSSUErSzMqB\ny4GHwli+iEhvhbJN0t0bgRyeJyUikh0640ZEClbQLEC33HILVVVVzJ49m6lTp/Kxj30scFq1/lJJ\nikhB6mmqNIBvfOMbrFq1ik2bNnHDDTewYMECDhw4kNUcKkkRyYlcT5V2rBtuuIErrriC++67L6uv\nI+yDyUUkD6p/WZ2z517956u7vS2XU6V1Zc6cOWzYsGGAr+j9NJIUkZzJ91RpuTjNWiNJEcmZoKnS\nli9fzvjx47nlllv6NVVaV1auXMncuT2ejt0nKkmRk8DKT6084aZKO9aDDz7Ik08+yY9+9KOs5Qet\nbotIDuVyqjSA22+//eghQPfeey/PPvssI0eOzOprCHWqtN7QVGldU5auKct7NFXaewYyVZpGkiIi\nAVSSIpITEydO/MBUaccjlaSISACVpIhIAJWkiEgAlaSISACVpIgUrN5OldbxkYuLBqokRaQg9WWq\ntI6PXByXqpIUkZzI91RpuaJzt0VOAptmnZmz5565YX23t+V6qrTbb7+de++9F4ChQ4eydOnSbL40\nQCPJXkklEjQeOkTj4VoS7X8Jw5JsbaWhpoam2iMkO82cIlKIcj1VWufV7VwUJGgk2S135+DWrWx4\n5ln2rF2HRaPYoHLSR+oYPHo0My9fyPizZxON5f4t9EyGvRs2sv7pZziwZQuRTlmGnzqBmYsWccoZ\npxOJ6G+eFJZ8T5WWCyrJLrTU1fHcT++ifv8B0skkuEM6jSWL8XSaI7t3s/z+X7Pi1w9wyeLPM2rK\nlJxlqd+/n6V3/pREYyOp1lYA0p2yHNyylVd+cQ+xkmLmf/lLDDnllJxlkePX1DWrT/ip0nJFQ49j\nNNfV8cQPfsiR3XtIJxJtBdmFVGsryeZmnv/JXexZ1/02mYE4sncvT/7wRzQdPny0ILvL0nykjqd+\ndDuHd+7KSRaR/sjXVGkdH9u2bcv6a9BUaZ1kMhme+Md/ov7AATyT+cDtVlGBd/FXrSge54q//iaD\nR43KWpZkSwuPfvd/0NrQ0OXt3WWJl5Vxzc1/R3EeV1nCnhKsM2V5j6ZKe4+mSsuSvevX01Rb22VB\nBkmnUqx/6umsZtm67DVSiUSfH5dKJtn80stZzSJyMgulJM2s0sweMLMNZrbezC4MI8ex1j/9TOBq\nbXc8k2HHijdINmdnb7O7s+GZZ9tW9/sok0yycelzZPpY9CLZpqnSBuafgSfcfQZwNpCbjXp90FxX\nR8227f1/AjN2rlqVlSyHtu8g0djY78enk0kObHonK1nk+Fbom9PyYaDvQd5L0syGAJcCdwO4e8Ld\ns3/CZR81HTpMdADbStKJBI01NVnJ0njoEBxz6ERfuDuNhw9lJYscv0pKSqipqTmpi9LdqampoaSk\npN/PEcYhQJOAA8C/m9nZwArga+5+dOhkZouBxQBVVVU5OWn9WEfq66G8HIvFur2PlZUFPkdTIpGV\nrPXNTTCoHIvH+5XFiopoaG7Jy/sGhH6IRmfK8p6KigoOHTrEvn37yGQyBXEcbRg5YrEYw4YN6/fv\nQxglWQTMAb7q7svM7J+BvwH+vuMO7r4EWAJte7fzsoewqQlvaMB72CbZ1R5lAItEqBg0KCt7MxsH\nD4HGJryHM2q6y0JxMUMqKvK6Z7VQ9iiDsnQ2YsQIIPw97R0KJUdfhPGnZRewy92XtX/+AG2lGarB\no0djA/gLFykqYvS0aVnJMmLyJDKpVL8fn0mnGTnltKxkETnZ5b0k3X0vsNPMprd/aSHQ/VxIeRKJ\nRpl66SVE+rldsqyykuETT81KluLycqrOPLPf2yVHT5tK2XH211qkUIW1keKrwC/N7C1gNvCPIeV4\nn6mXXNyvx0XjcWZevjCrWWYsvKxfO5Ki8TgzFmY3i8jJLJSSdPdV7j7X3c9y94+6++EwchyrdMgQ\nZl/3UaIBO0yOFSkqYsSkiUw877ysZhl+6qmcdvE8ovHudyQdKxqPceo5cxg1NXfnkoucbMLf3VVg\npl16CbOuuopowF7uDtF4jBGTJ3HJ4ptysseu+qN/ysRzz+1VaUfjccadfTZzb7zhAzOviEj/aRag\nLsy8fCFDx1ex+g+PU/vuu2TS6fedqlhUXEyspITpCxcw7dJLiESjOclhkQjn3ngDo6dNY83jT9B4\n6NB7sxIBmBGNxSgdMoQzrrqSiefOVUGKZJlKshtjZsxgzIwZ1O/fz5ZXX6Ox5iCtQEVZGeNmn83o\nadPyVkgT5lQzYU41h3buZNvry9tmBYpGGTxoEKfOncuwUyeoHEVyRCXZg4pRozj7I9cC4R/jNWz8\neIaNH18QWUROFtomKSISQCUpIhJAJSkiEkAlKSISQCUpIhJAJSkiEkAlKSISQCUpIhJAJSkiEkAl\nKSISQCUpIhJAJSkiEkAlKSISQCUpIhJAJSkiEkDzSfZCsqWFlvp6GurqKIlGKamoCC1LoqmJloYG\nGuvrKY3FKC4vDy1LS30DicZGGpsaKSsuJl5aGlqWQpFx51B9Kw0tSdItLZSWpymO5WbmeskPlWQ3\n3J39b29i/TPPsP/tTW2Xmh1UTuZIHeXDhjHz8oVMmDOHoj5cNKy/Muk0u9euY/3TT3N4x873stQe\nYUjVWE5ftIiqs87M2WUkOksnk+xctYr1Tz1D/f7978sy4rTJzFy0kDHTpw/oGubHo9rGVh5fuYOH\nl22jOZEiGjGGFWc40GzMP2MsHzt/EpNGDw47pvSDecf1UgpUdXW1r1y5Mq/LbD5yhKU//knbZRJa\nW49+3Soq8Pp6oO06NwAXf/4vGTNjepfPkw1H9u7luR//hGRLS2CWaCzG/C9/kaHjxuUsy4HNm3nh\nZ/+KZzKBWUoqKrjsq1+mfNiwnGXpThgztv922VbufnYDAInUe9dCGlnqHGg2IgZF0QjVk0bw3z8+\nJ5SRZaHMZF8oOQDMbIW7z+3pfifXn/teaKqt5Ykf/C/q9+9/XxEcK9XaSqq1lT8u+VfeXb0mJ1lq\nd+/mqVtvo/nIkR6ztDY08Mzt/0zN9u05ybJ340aeu/OnJJube8zSeKiG//zBD2k4WJOTLIXk/zz/\nNv+2dCOJVOZ9BdlZxtvKc+XWg/z/v3iFRCqd55QyECrJTjKZDEv/5cckGpved3XEIOlkkpf/4xcc\n2bs3q1kSzc0s/d8/DiykY6USCZ6786e01DdkNUtDTQ0v/uvP267U2AuecZLNzTz7v/+FdDKV1SyF\n5MX1e/jNy5tpTfau9BKpDNsO1HPr797McTLJJpVkJ3vWrqP5SF2vC7JDJpVi/ZNPZzXL1mXLSPWy\nlI7Nsvmll7KaZeMzS0n3cfTj7iSamtj15olZCO7O3c9soLWb0WN3EqkML2/cx/4jzTlKJtkWSkma\n2TYzW21mq8xseRgZurL+6af7NHLr4JkMO1etItGcnR98d2fDM0tJJxJ9fmw6mWTjc8+T6WPRdyeV\nSLBl2TI83fdVxFRrK+ufzu4fj0KxcXctNQ19/1np8PvludksItkX5kjyMnef3ZsNp/nQfOQIh3bs\n7P8TmLFz5aqsZDm0fTvJARRuJpVi/9ubspJl9+o1A7qmd/3+A9QfOJCVLIXk0eXb+71tMZnO8Ngb\nO7KcSHJFq9vtmg7XEi3q/xFR6USCxkOHspKl8dDhAT3e3Wk6PLDnOJrl8OFeb4vsSqQoStMAX08h\nevdQEwM5MKSxJUk6U9hHlkibsI6TdOBpM0sDP3P3JZ1vNLPFwGKAqqoqamtrcx7oSH09lJdjsVi3\n97GyssDnaE4kspK1vrkJBpVjAcdgBmWxoiIaWlqykqUpkWh7XwLuE/i+xOPUNTRQnIfvIUB9+6FI\nuVZMKyNLg0uusrj72w2j5tAh4kX5ORwoX+9LTwolR1+EVZIXu/u7ZjYKeMrMNrj7Cx03tpfmEmg7\nTjIvx1U1NeENDXgP2yS9m2+yRSJUDBqUlWPAGocMgcYmvKWlX1koLmbI4MFZyVJRXo41N5NJBe+l\n7i6LlxQzdNiwvB4bl49lWbyMA81NPd7vQHPXf16iEWPk8GED2pTRV4VyfGKh5OitUFa33f3d9n/3\nAw8D54WRo7PBo0cP6IyVSFERY2bOyEqWkZMn91hKQTLpNKOmTslKljEzpg/sF9lh6LiqrGQpJBfP\nGEPJAA4KnzU+vwUp/Zf3kjSzcjOr6Pg/cAWQm6Ox+yASjTL1Ty5tO82uH8qHDWXY+PFZyRIvK2P8\n2Wf3+5dozIzplA7Ozilwg0ePZsjYU/r12Eg0ymkXXUg0YBPG8WrBmVX0d4tiaTzKJy6anNU8kjth\njCRHAy+a2ZvAa8Af3P2JEHJ8wJSL5/WrmKLxODMXLcpqlukLL+tXYUfjcWYsXJDVLDMXLerXOeoW\niTD10kuzmqVQlMaLWDCriqJo339eSuNFnHPayBykklzIe0m6+xZ3P7v94wx3/5/5ztCd0sGDqf74\nx/o08onEihg15TQmnpvdI5mGjR/PtD+5lGi891mi8TiTzj+PUVOys6rdYdzZZ3HK6TP79L5E43HO\nvPYaBo0YntUsheSmRTMYUVFCNNL7oiwuinDzJ84holXt44YOATrGlHkXcea11/SqEKLxGKOmTGHe\nX34uJ7PenPWRD3PaRRcR7cUoLhqPM2FONedc//Gs5zAzLvjzP+OUmTN7lyUW4/TLFzFjwWVZz1JI\nykti3PbZixg1pJRYNPj7HzEoiUX5zg1zmTluaJ4SSjZoFqBu7Nu0iTWPPU7Ntu3gTiadPjrbTVE8\nTry8nJmXL+K0eRcRyfG0YLvefIs1TzxB/b79ZNJpPJNpy9LQQDRWxKDhIzjjyisYP6c6pzsDPJNh\ny6vLWP/U0zTX1R09I8gqKqCpCYsYQ8eNY9bVV3NKlnZi9VUYs8w0tiZ54OUtPLJ8G+mM05xoO8h8\nZKlTl4yScbho+mg+/SfTmDBiUF6zdSiU2XcKJQf0fhYglWQPGg7WsO3112k4WEMrTkVZGePOPpuR\np03O+97J2t272b58Bc21tSQiEQZXVHDqnDkMHZ+76dG64u7UbNvOzlWraDlSRyoWo7JyCBPPnUvF\nqFF5zXKsMH8JU+kMr7y9jze31VDXlGBIPM3YUSNYeFYVg0tzP+9okEIpp0LJASrJnCikb7CydE1Z\nulYoWQolB2g+SRGRrFBJiogEUEmKiARQSYqIBFBJiogEUEmKiARQSYqIBFBJiogEUEmKiARQSYqI\nBFBJiogEUEmKiARQSYqIBFBJiogEUEmKiARQSYqIBFBJiogEUEmKiARQSYqIBOhTSZrZaWa21My2\nmNltZlbS6bbXsh9PRCRcfR1J3gk8BHwCGAk8bWYd18js/ZXrRUSOE30tydHu/i/uvsLdPwM8BTxl\nZhVAry+7aGZRM1tpZo/2cfkiInlV1Mf7l3b+xN2/a2Zp4EmgL1dd/xqwHhjcx+WLiORVX0eSm8xs\nQecvuPs/AE8AU3rzBGY2DrgG+Hkfly0iknc9jiTNbI67v9H+6WfoYrW6fUT5m14u8w7gW0BFwDIX\nA4sBqqqqqK2t7eVT51Z9fX3YEY5Slq4pS9cKJUuh5OiL3qxuLzWzj7r7Unfvtq3cfV1PT2Rm1wL7\n3X2Fmc0PeK4lwBKA6upqr6ys7EXM/FCWrilL15TlgwolR2/1ZnX7PuAxM/v4sTeY2cVm9mIfljcP\n+IiZbQPuBxaY2b19eLyISF71WJLu/kXg+8D9ZvZfAcxslpn9HngBGNrbhbn7t919nLtPBG4EnnX3\nT/cruYhIHvRq77a7f8/MdgM/MbNP0jYi3Al8Drgnh/lERELVq5I0s6HAVCANXAK8DMx391R/F+zu\nzwHP9ffxIiL50OPqtpndAmwFvgz8iLbR41zgtpwmExEpAL0ZSf4tbcc0fs/d9wKY2U7gITMbDXza\n3ZM5zCgiEprelORMd9/c+Qvu/oyZXQY8RtuB5AtzEU5EJGy92bu9uZuvvwFcDEzMciYRkYIxoPkk\n3f0d4KIsZRERKTgDnnTX3fdlI4iISCHSzOQiIgFUkiIiAVSSIiIBVJIiIgFUkiIiAVSSIiIBVJIi\nIgFUkiIiAVSSIiIBVJIiIgFUkiIiAVSSIiIBVJIiIgFUkiIiAVSSIiIBVJIiIgFUkiIiAVSSIiIB\nVJIiIgHyXpJmVmJmr5nZm2a21sy+m+8MIiK91ZvrbmdbK7DA3RvMLAa8aGaPu/urIWQREQmU95J0\ndwca2j+NtX94vnOIiPRGGCNJzCwKrACmAHe6+7Jjbl8MLAaoqqqitrY2/yG7UF9fH3aEo5Sla8rS\ntULJUig5+iKUknT3NDDbzCqBh81slruv6XT7EmAJQHV1tVdWVoYRs0vK0jVl6ZqyfFCh5OitUPdu\nu3stsBS4KswcIiLdCWPv9sj2ESRmVgpcDmzIdw4Rkd4IY3X7FOAX7dslI8Cv3f3REHKIiPQojL3b\nbwHV+V6uiEh/6IwbEZEAKkkRkQAqSRGRACpJEZEAKkkRkQAqSRGRACpJEZEAKkkRkQAqSRGRACpJ\nEZEAKkkRkQAqSRGRACpJEZEAKkkRkQAqSRGRACpJEZEAKkkRkQAqSRGRACpJEZEAoVx3W0Ryr6k1\nxe5DjTQnUniymVhJOeUlsVCy1DUn2FfbzJHaIwxtMaqGD6IkFg0lS1+pJEVOMFv31fHgsq08v3Y3\nRZEIZjCsOMO+ptXMmzGGj18wmamnDMl5Dndn3a7DPPDKFl5/5wCxogjDizPUtERIu3P5WVVcd/4k\nxg0flPMsA6GSFDlBpDMZbvv9W/xx3R6SaSfjToIMAGURJ5Eynl+7m5c37uPc00by19fNJl6Um9Fc\nSyLFLb9ewfpdh2lNpnEgmc5QHnGaEm2ZHl+5kyff3MU1cyaw+IrTiZjlJMtAaZukyAkgnXG+83+X\n88f1e2hNZci4d3m/jENrMs3r7+znb3/5Gql0JutZWpNp/uo/XmHtzkO0tBdkd5kTqQyPrdzJjx55\nE+8mc9hUkiIngHuff5u3ttfQmuxd6bWmMry9p5YlT63Pepbbfv8mO2saSKR6mSWZ5o/r9/LI69uy\nniUbVJIix7nWZJqHlm3tdUG+97gMT6zcQWNrMmtZaupbeGnDvl4X5HtZ0tz3x3dIZwpvNJn3kjSz\n8Wa21MzWmdlaM/tavjOInEheWLeH/m7NMzOeeevdrGV5dMX2fj+2NZVmxeYDWcuSLWGMJFPA/+fu\npwMXAF82s9NDyCFyQnjk9W00J9P9emxLMs0jy7dlLcvjb+wg2c/tnM2JNL/PYpZsyXtJuvsed3+j\n/f/1wHqgKt85RE4UB+tbBvT4ww2JLCWBuuaBrbrvPzKw15ILoR4CZGYTgWpg2TFfXwwsBqiqqqK2\ntjbv2bpSX18fdoSjlKVrJ2OWIbE00dLgbXmVxd3fXlKUzsrvmLszrDh4FBmUA2BQNFkwv+8dQitJ\nMxsEPAh83d3rOt/m7kuAJQDV1dVeWVkZQsKuKUvXlKVr+ciStGIONKd6vN+B5q63XI6oiGctZ0O6\niOZE8Kp/dzkARg4vLajvH4S0d9vMYrQV5C/d/aEwMoicKC494xRi0f79KhdFjItnjslalvOmjCLS\nz71IJbEol80am7Us2RLG3m0D7gbWu/tt+V6+yInm2nNO7fdjIxHjT8+dmLUs1184mVg/z+Jxdxae\nVXi7J8IYSc4DPgMsMLNV7R8fCiGHyAlheEUJcyaPINrHIVw0YkwfW8nYYeVZyzJtbCVjKkv7fEhS\nLGrMP2Ms5cXhTMARJIy92y+6u7n7We4+u/3jsXznEDmR/NWHz2JIWbzX5z8bUF4S42+uq856lps/\ncQ6lxb3f3RGNGCMHl/KFKwvzSECdcSNyAqgsL+aOv7iIEYOLiRcF/1rHohGGDirm9s9eyIjBJVnP\nMm74IG79swuoKIlR1MPotrgoQtWwcm777EUFOYoEzQIkcsIYXVnGXYsv5Xevb+PhZVtJpjO0JN6b\nYKI0HiUaifCRc0/luvMmMbgsnrMsp40Zws/+66U8tGwrf1ixA/Cje70NKIlHKY0X8fELJnPtORMo\niRduFRVuMhHps/KSGP/lkqncMG8Kr7+zn81766hvTjC4KMWpVaM5f+ooivq5J7yvhleUcNOimfz5\n/Gm8vHEfOw820NrcQFl5BdOrKqmeNKJgp0frTCUpcgKKRowLpo3mgmmjAaitrQ3t+MN4UZT5Z4wN\nPUd/aZukiEgAlaSISACVpIhIAJWkiEgAlaSISACVpIhIAJWkiEgAlaSISACVpIhIAJWkiEgAlaSI\nSACVpIhIAJWkiEgAlaSISACVpIhIAM0neRxxd+r27aOlvp6GpmasuZnBY8Zgx8HEpSeLlmSa7Qfq\naWhJQrKJSUUlDBuU/Usk9EZDS5KdBxtoSqSwZDNTissYXJq72ciD1Da28u6hRurqjjC0EU4dOYjS\nAp6NvLPjI+VJLtnSwrbXl7Ph6WdoaWggEo1AWTmZ+nriZWXMWHgZk84/n3hpadhRT1o7Dzbw8Gtb\nefqtd4lGDAOGFWfY27iOWROG8YmLJudtJu5Ne47wwCubeXnjvqPX4x5WnGFf01rOmzKK6y+czIyq\nypz/cXV33tp+iN+8splVW2uIF0UYVpyhpjVCOp1h/qyxfOz8yUwcVZHTHANl7t7zvUJUXV3tK1eu\nDDsGEM6syge3bOX5u+4ik86QTiSOft0qKvD6egCi8ThmxiU3fZ7R06flNR8U1mzT+c7i7tz9zAZ+\n9/o20hknnXnv92lkqXOgua2ISuJRJowYxD/+l/OpKM3NBa+S6Qy3/m4Vr7y9n2QqTacoR7OYQXFR\nlNmThvPZeYHAAAAMWUlEQVTfPz6HeD+vkd2TptYUf3//67yz9witna6z0/k9iVjbRckuP3scX7pq\nVp8viTtQZrbC3ef2dD9tkyxgBzZvZumP7yTZ3PK+gjxWOpEg1drKCz9bwp71G/KY8OTm7vzzH1bz\nyPLtJFKZ9xXksVoSabbsq+Ord79IY0sy61nSGefmX73OKxv30Zp8f0G+P3PbJoE3thzkr//PMpLp\nTNaztCRSfP3fX2Lju7XvuxDZsTIOrakMT721ix/8diWFOmBTSRaolvoGnv/pz0gne/8LlU4meenu\nu2k6XJvDZNLhiVU7eXbNblqT6V7dP5V2Dta18D8ffCPrWf792Q2s3XWI1lTvSi+RyrB57xF++p9r\ns57lh797kz2Hm3pdwK3JDK++vZ+Hl23NepZsUEkWqM0vv4xn+v5XPpPO8PYLL+QgkXTm7tz7/KZe\nF2SHZDrD6h2HeLemMWtZWpJpfr98O63Jvv28tKYyPPXmrqyObA/UNbNs034SvSzro1mSaX714juB\no/Gw5L0kzezfzGy/ma3J97KPF5lMhrefe75Po8ijj02l2PziS6RTqRwkkw5vbq9p24PdD5mM8/Br\n2Rs1Pb92d78fa2Y8+daurGV5dPn2fj82mc7w+jv7s5YlW8IYSf4HcFUIyz1uHNyypV8F2cHd2bdx\nYxYTybEeW7GDlj6OIjukMs7TWSym37++rd9ZWpPpARXbsZ5YubPf2zmbE20j4kKT95J09xeAQ/le\n7vGk+UjdgB7v7gN+Dgm270jzgB7fkkiTytJOk5qG1gE9vrZxYI/vrH6Aq+4H6gb2vuZCQR4naWaL\ngcUAVVVV1NYWxo6I+vZDbnKtsaUZBpVj8e4P/LWysu5vKyqiKdGat/ctX+9Lb+QrS1kkycjS4O1n\nlcXd327A4drDxKIDPwRnSCxNdABZSooyWflZcXeGFQcXf1AOgIpoqmB+3zsUZEm6+xJgCbQdJ1ko\nx+ABeTkGr6lyKNbYhLe0BN7PuyuE4mKGDBmS1+MFT7bvUTRexoHmnne+dBwTeKyiiDFy+PCsZElG\nijnQ3PM26O6yjBgcz9p71pCO0ZwIztJdDoBRI0oL6mcJtHe7II2cchqZdP+2MQFk0mlGT8v/QeUn\nk8tmjaU03r9RoAHnnDYya1nmnzH26Jk1fVUUNS6deUrWslwwbVS/zyoqiUVZMGts1rJki0qyAMWK\ni5l47lws0vdvj5kx7qyzKC4vz0Ey6XDJ6f0vluJYlE9cODlrWa45Z0K/Hxsx40/PnZi1LB+/YDKx\nov7Viruz4MxxWcuSLWEcAvQr4BVgupntMrO/zHeG48G0y+YT6cf2qkhRETMWLshBIuksXhTlQ3Mm\nEO9jIRhQWV7MrAnDspZl2KAS5p42kqI+ntYXjRgzxw1lzNDut2/31dRThnDK0DL6OpiMRSMsOLOK\nsuLC2wIYxt7tT7r7Ke4ec/dx7n53vjMcD4aMGcNZH7mWaLz35/lG43FOv+Jyhk0Yn8Nk0uGzl01n\n/PBBxKK9b4SSeJT/cePcrE8u8VcfPovK8uJer+pGDCpKY/zNdbOzmgPgO584h7I+zPBTFDHGVJby\nhStOz3qWbNDqdgGbPn8+sz70IaKxnosyGo8xY+ECTr/yijwkE2gbTf6vP7uAyaMHUxwLHvXHosag\nkiJ++GcXMmFk9me9GVwW547PXcToytIeR7fxogjDKkq44y/m5WQat7HDyrntsxcxuCze47bS4liE\n8SMG8aPPXlSwU6dpFqA+CGu2m4Nbt7LuyafZu2EDZkY6mcQqKoi0tODujJo6hdOvuJxRU6bkPRuc\n3LMAQduZIk+t2smvX9nC4YZWkum2yS5GlTr16SLMjGvOmcB1501ieEVu55Zsak3xhze289CrW2lO\npI5OvDG61KlLFxEvinLd+RP58NyJDCrJzWxEHWobW/nda9t4ZHnbDEmtyTTDS5xDrRFi0QiDy+J8\n4sLJXDl7fI9/ZHKht7MAqST7IOwyaK6rY9ebb9FSV0erO0MGD6bqrDMpC7mgwn5fOgszi7uz/t1a\n1u44RH1LkrJIkqpRI7hg+uh+733ur4w7K7ce5J09dTS2JBlUlGTi2NGcc9rIvE9JlkpneG3TfnYc\nbKC1uYHS8gpmVlUya8KwUCeMVknmgMqga8rSNWUp3Byg+SRFRLJCJSkiEkAlKSISQCUpIhJAJSki\nEkAlKSISQCUpIhJAJSkiEkAlKSISQCUpIhJAJSkiEkAlKSISQCUpIhJAJSkiEkAlKSISQCUpIhJA\nJSkiEkAlKSISQCUpIhJAJSkiEkAlKSISIJSSNLOrzGyjmb1jZn8TRgYRkd7Ie0maWRS4E7gaOB34\npJmdnu8cIiK9EcZI8jzgHXff4u4J4H7gT0PIISLSo6IQllkF7Oz0+S7g/M53MLPFwOL2T1vNbE2e\nsvVkBHAw7BDtlKVrytK1QslSKDkApvfmTmGUZI/cfQmwBMDMlrv73JAjAcrSHWXpmrIUbg5oy9Kb\n+4Wxuv0uML7T5+PavyYiUnDCKMnXgalmNsnM4sCNwCMh5BAR6VHeV7fdPWVmXwH+E4gC/+buawMe\nsiQ/yXpFWbqmLF1Tlg8qlBzQyyzm7rkOIiJy3NIZNyIiAVSSIiIBCrokC+X0RTP7NzPbXwjHa5rZ\neDNbambrzGytmX0tpBwlZvaamb3ZnuO7YeQ4JlPUzFaa2aMh59hmZqvNbFVvDzPJYZZKM3vAzDaY\n2XozuzCkHNPb34+Ojzoz+3oYWdrzfKP953aNmf3KzEq6vW+hbpNsP33xbeBy2g44fx34pLuvCyHL\npUADcI+7z8r38o/Jcgpwiru/YWYVwArgo/l+X8zMgHJ3bzCzGPAi8DV3fzWfOY7J9FfAXGCwu18b\nYo5twFx3D/2gaTP7BfBHd/95+9EkZe5eG3KmKG2H/Z3v7ttDWH4VbT+vp7t7s5n9GnjM3f+jq/sX\n8kiyYE5fdPcXgENhLPtY7r7H3d9o/389sJ62s5jyncPdvaH901j7R2h/cc1sHHAN8POwMhQaMxsC\nXArcDeDuibALst1CYHMYBdlJEVBqZkVAGbC7uzsWckl2dfpi3sugkJnZRKAaWBbS8qNmtgrYDzzl\n7qHkaHcH8C0gE2KGDg48bWYr2k+xDcsk4ADw7+2bIX5uZuUh5ulwI/CrsBbu7u8CtwI7gD3AEXd/\nsrv7F3JJSgAzGwQ8CHzd3evCyODuaXefTdtZU+eZWSibIszsWmC/u68IY/lduLj9fbka+HL75pow\nFAFzgJ+6ezXQCIQ6NWH7Kv9HgN+EmGEobWulk4CxQLmZfbq7+xdySer0xW60bwN8EPiluz8Udp72\nVbilwFUhRZgHfKR9W+D9wAIzuzekLB0jFdx9P/AwbZuOwrAL2NVphP8AbaUZpquBN9x9X4gZFgFb\n3f2AuyeBh4CLurtzIZekTl/sQvsOk7uB9e5+W4g5RppZZfv/S2nbwbYhjCzu/m13H+fuE2n7OXnW\n3bsdGeSSmZW371CjfdX2CiCUoyLcfS+w08w6ZrtZCOR9x+cxPkmIq9rtdgAXmFlZ++/TQtq27Xep\nIGcBgn6dvpgzZvYrYD4wwsx2Ad9x97vDyELbqOkzwOr27YEAf+vuj+U5xynAL9r3VEaAX7t7qIfe\nFIjRwMNtv3sUAfe5+xMh5vkq8Mv2gcYW4C/CCtL+R+Ny4AthZQBw92Vm9gDwBpACVhJwimLBHgIk\nIlIICnl1W0QkdCpJEZEAKkkRkQAqSRGRACpJEZEAKkkRkQAqSRGRACpJOe6Y2RQzS5rZ9475+k/N\nrN7MCuKSpXJiUEnKccfd36FtSrSvm9lwADO7GfgccJ27hzrRrZxYdMaNHJfaJx9+B/gJsBH4GW2T\nMv861GBywtFIUo5L7r6HtjkkvwrcBfy3zgVpZn9vZm+bWcbMPhpWTjn+qSTleLYJKAZecfc7j7nt\nKdqmbnsh76nkhKKSlOOSmS2kbRX7FWCemZ3V+XZ3f9Xdt4QSTk4oKkk57pjZHNoms/05bVPY7QC+\nH2YmOXGpJOW4YmZTgMeBJ4Gvtl8k7rvAh0K8TIKcwFSSctwwszG0leN64FPu3nHRr3tomxX9n8LK\nJieugp2ZXORY7ZcjmNzF19PAzPwnkpOBjpOUE5KZ3QJ8HhgJ1AMtwAXuvivMXHL8UUmKiATQNkkR\nkQAqSRGRACpJEZEAKkkRkQAqSRGRACpJEZEAKkkRkQAqSRGRAP8Pkh4GQnTPIPsAAAAASUVORK5C\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11630fcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([[1,2,1], [1,1,5], [1,2,5], [1,3,5], [1,1,6], [1,2,6], [1,5,1], [1,6,1], [1,7,1], [1,6,2], [1,7,2], [1,5,5]], dtype=float)\n",
    "y = np.array([1 if ii < 6 else 0 for ii in range(X.shape[0])], dtype=float)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5,5))\n",
    "ax.scatter(X[:,1], X[:,2], color=[\"#a76c6e\" if ii < 6 else \"steelblue\" for ii in range(X.shape[0])], s=250)\n",
    "ax.plot([0,8], [-1,-1], lw=3, label=\"part B\") # TODO \n",
    "ax.plot([0,8], [-1,-1], lw=3, label=\"part C\") # TODO \n",
    "ax.plot([0,8], [-1,-1], lw=3, label=\"part D\") # TODO \n",
    "ax.plot([0,8], [-1,-1], lw=3, label=\"part E\") # TODO \n",
    "ax.grid(alpha=0.25); ax.set_xlim([0,8]); ax.set_ylim([0,8]); ax.set_xlabel(r\"$x_1$\", fontsize=16); ax.set_ylabel(r\"$x_2$\", fontsize=16)\n",
    "ax.legend(loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Suppose you use the the standard Logistic Regression decision rule such that a query point ${\\bf x}$ is predicted to be $\\hat{y} = 1$ if $p(y = 1 \\mid {\\bf x}) \\geq 0.5$ and $\\hat{y} = 0$ otherwise. Describe the decision boundary of such a classifier.  How could you plot the decision boundary in a 2D feature space like the one shown above?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Suppose you learn a Logistic Regression classifier from this training set by minimizing the negative log-likelihood \n",
    "\n",
    "$$\n",
    "NLL(\\boldsymbol{\\beta}) = -\\displaystyle\\sum_{i=1}^n \\left[y_i \\log \\textrm{sigm}(\\boldsymbol{\\beta}^T{\\bf x})       + (1-y_i)\\log(1 - \\textrm{sigm}(\\boldsymbol{\\beta}^T{\\bf x}))\\right]\n",
    "$$\n",
    "\n",
    "Describe a possible decision boundary that you could learn as a result.  Plot the decision boundary on the graph above and label it \"part B\". How many training examples does your learned decision boundary misclassify? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Suppose you learn a Logistic Regression classifier from this training set by minimizing the negative log-likelihood with the parameter $\\beta_0$ so strongly regularized that it approaches zero, and the other parameters unregularized.  \n",
    "\n",
    "$$\n",
    "\\textrm{Loss}(\\boldsymbol{\\beta}) = NLL(\\boldsymbol{\\beta})+ \\lambda \\beta_0^2\n",
    "$$\n",
    "\n",
    "Describe a possible decision boundary that you could learn as a result.  Plot the decision boundary on the graph above and label it \"part C\". How many training examples does your learned decision boundary misclassify? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Suppose you learn a Logistic Regression classifier from this training set by minimizing the negative log-likelihood with the parameter $\\beta_1$ so strongly regularized that it approaches zero, and the other parameters unregularized.  \n",
    "\n",
    "$$\n",
    "\\textrm{Loss}(\\boldsymbol{\\beta}) = NLL(\\boldsymbol{\\beta})+ \\lambda \\beta_1^2\n",
    "$$\n",
    "\n",
    "Describe a possible decision boundary that you could learn as a result.  Plot the decision boundary on the graph above and label it \"part D\". How many training examples does your learned decision boundary misclassify? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: Suppose you learn a Logistic Regression classifier from this training set by minimizing the negative log-likelihood with the parameter $\\beta_2$ so strongly regularized that it approaches zero, and the other parameters unregularized.  \n",
    "\n",
    "$$\n",
    "\\textrm{Loss}(\\boldsymbol{\\beta}) = NLL(\\boldsymbol{\\beta})+ \\lambda \\beta_2^2\n",
    "$$\n",
    "\n",
    "Describe a possible decision boundary that you could learn as a result.  Plot the decision boundary on the graph above and label it \"part E\". How many training examples does your learned decision boundary misclassify? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [30 points] Problem 3: SGD for Regularized Logistic Regression \n",
    "***\n",
    "\n",
    "In this problem you'll implement a Logistic Regression class that trains a classifier using Stochastic Gradient Descent with $\\ell_2$-Regularization. In Problem 4 you'll use this class to do document classification. Here your job will be to implement the following methods: \n",
    "\n",
    "- `train`: Takes in learning rate, regularization strength, and number of epochs to do, and learns model parameters using SGD \n",
    "- `predict`: Takes in a matrix of examples and predicts binary labels in $\\{0,1\\}$\n",
    "- `accuracy`: Takes in a matrix of examples and true labels, makes predictions, and returns accuracy as value in $[0,1]$ \n",
    "\n",
    "Note that you should assume that all features have been prepended with a $1$ so that each example is the same length as the parameter vector `beta`. \n",
    "\n",
    "There are some optional methods that you may implement if you like which might make your life easier in later problems.  These will not be unit-tested or graded though.  They are \n",
    "\n",
    "- `predict_proba`: Takes in a matrix of examples and estimates $p(y=1 \\mid {\\bf x})$ for each example. \n",
    "- `mean_loss`: Takes in a matrix of examples and true labels and evaluates the negative log-likelihood  \n",
    "\n",
    "Finally, the method `best_text_features` will not be needed until **Problem 4**. \n",
    "\n",
    "The section below the class skeleton contains more details as well as unit tests. Note that the unit tests are based on a subset of the toy data in **Problem 2**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogReg:\n",
    "    \"\"\"\n",
    "    Class to train a logistic regression classifier on the training data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_valid=None, y_valid=None):\n",
    "        \"\"\"\n",
    "        Initialize classifier \n",
    "        \n",
    "        :param X_train: ndarray of training features (with column of 1s prepended)\n",
    "        :param y_train: ndarray of training labels {0,1}\n",
    "        :param X_valid: ndarray of validation features (with column of 1s prepended)\n",
    "        :param y_valid: ndarray of validation labels {0,1}\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X_train = X_train \n",
    "        self.y_train = y_train \n",
    "        \n",
    "        self.X_valid = X_valid \n",
    "        self.y_valid = y_valid \n",
    "        \n",
    "        # Array of logistic regression weights \n",
    "        self.beta = np.random.randn(self.X_train.shape[1])\n",
    "        \n",
    "        # list for storing loss function histories \n",
    "        self.train_history = [] \n",
    "        self.valid_history = [] \n",
    "        \n",
    "    @staticmethod\n",
    "    def sigmoid(z, threshold=20):\n",
    "        \"\"\"\n",
    "        Evaluate the sigmoid function \n",
    "        :param z: argument of sigmoid function \n",
    "        :param threshold: threshold parameter to prevent over/underflow \n",
    "        \"\"\"\n",
    "        \n",
    "        if np.abs(z) > threshold:\n",
    "            z = np.sign(z) * threshold\n",
    "            \n",
    "        return 1.0 / (1 + np.exp(-z))\n",
    "    \n",
    "    def train(self, eta=0.01, lam=0.0, num_epochs=10):\n",
    "        \"\"\"\n",
    "        train LogReg model using SGD with regularization \n",
    "        \n",
    "        :param eta: the learning rate \n",
    "        :param lam: the regularization strength\n",
    "        :param num_epochs: number of epochs to perform in training \n",
    "        :return : returns nothing, just updates weights\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO \n",
    "        \n",
    "        self.beta = self.beta \n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        predict probability p(y = 1 | x) for each row of X (this function is optional)\n",
    "        \n",
    "        :param X: ndarray of features \n",
    "        :return : ndarray of probabilities \n",
    "        \"\"\"\n",
    "        return np.zeros(X.shape[0])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict binary labels {0,1} for each row of X\n",
    "        \n",
    "        :param X: ndarray of features \n",
    "        :return: ndarray of binary labels {0,1}\n",
    "        \"\"\"\n",
    "        return np.zeros(X.shape[0]) # TODO \n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        report accuracy of prediction\n",
    "        \n",
    "        :param X: ndarray of features \n",
    "        :param y: associated true labels\n",
    "        :return: accuracy as a float in [0.0,1.0]\n",
    "        \"\"\"\n",
    "        return 0.0 # TODO  \n",
    "    \n",
    "    def mean_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        report mean log-likelihood (this function is optional)\n",
    "        \n",
    "        :param X: ndarray of features \n",
    "        :param y: associated true labels\n",
    "        :return: average log-likelihood\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import log_loss\n",
    "        return 0.0\n",
    "    \n",
    "    def best_text_features(self, vocab):\n",
    "        \"\"\"\n",
    "        Print 10 best features for each class \n",
    "        \n",
    "        :param vocab: list of vocab words\n",
    "        :return: returns nothing \n",
    "        \"\"\"\n",
    "        class0 = [0]*10 # TODO \n",
    "        class1 = [0]*10 # TODO \n",
    "        print(\"\\nbest words for class 0\")\n",
    "        print(\"----------------------\")\n",
    "        for ind in class0:\n",
    "            print(vocab[ind])\n",
    "\n",
    "        print(\"\\nbest words for class 1\")\n",
    "        print(\"----------------------\")\n",
    "        for ind in class1:\n",
    "            print(vocab[ind])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Implement the `train` method so that it performs **unregularized** SGD updates of the model parameters by minimizing the negative log-likelihood loss function discussed in lecture:  \n",
    "\n",
    "$$\n",
    "\\textrm{NLL}({\\bf \\beta}) = -\\displaystyle\\sum_{i=1}^n \\left[y_i \\log \\textrm{sigm}(\\boldsymbol{\\beta}^T{\\bf x})       + (1-y_i)\\log(1 - \\textrm{sigm}(\\boldsymbol{\\beta}^T{\\bf x}))\\right] \n",
    "$$\n",
    "\n",
    "\n",
    "Note that your SGD updates should be vectorized, utilize Numpy routines as much as possible, and not make any assumptions about the number of features. When you think you're done, execute the following code cell to perform three unit tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i tests/tests.py \"prob 3A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Update your implementation of the `train` method so that it performs **regularized** SGD updates of the model parameters to minimize the regularized loss function discussed in lecture\n",
    "\n",
    "$$\n",
    "\\textrm{Loss}({\\bf \\beta}) = -\\displaystyle\\sum_{i=1}^n \\left[y_i \\log \\textrm{sigm}(\\boldsymbol{\\beta}^T{\\bf x})       + (1-y_i)\\log(1 - \\textrm{sigm}(\\boldsymbol{\\beta}^T{\\bf x}))\\right] + \\lambda\\displaystyle\\sum_{k=1}^p \\beta_k^2\n",
    "$$\n",
    "\n",
    "Note that you should **NOT** regularize the bias parameter $\\beta_0$. When you think you're done, execute the following code cell to perform two unit tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i tests/tests.py \"prob 3B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Implement the `predict` function to take a matrix of examples and use the learned parameters to return a vector of predictions of $\\{0,1\\}$ for each example. When you think you're done, execute the following code cell to perform one unit test. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i tests/tests.py \"prob 3C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Implement the `accuracy` method to take a matrix of examples and a vector of true labels, make predictions, and return the accuracy of those predictions as a decimal value in $[0,1]$. Execute the following code cell to perform one final unit tests. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i tests/tests.py \"prob 3D\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 4: Baseball vs Hockey \n",
    "***\n",
    "\n",
    "In this problem you will train a Logistic Regression classifier to determine if a document is talking about baseball or hockey. The following code cell will load training and validation sets, as well as a list that encodes the map from feature index to particular words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = gzip.open(\"data/baseball_hockey.pklz\", 'rb')\n",
    "X_train, y_train, X_valid, y_valid, vocab = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Look at the encoded features in `X_train` or `X_valid`.  Which of the text models discussed in class do these features represent? Briefly justify your response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: There are two additional files in the data directory called `positive_raw` and `negative_raw`.  These are a subset of the actual documents that were cleaned and featurized to obtain our training and validation data.  The documents in `positive_raw` correspond to examples with true label $y=1$ and the documents in `negative_raw` correspond to examples with true label $y=0$. Inspect some of the documents and decide which label corresponds to documents about baseball and which label corresponds to documents about hockey. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Use the class you wrote in **Problem 3** to train a logistic regression classifier to predict baseball vs hockey and report accuracy on the training and validation set.  Do you see any signs of overfitting?  \n",
    "\n",
    "**Hint**: You won't need to run very many epochs before convergence on this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Modify your code so that it periodically records accuracy on the training and validation sets throughout the training process (try recording after every $50$ training examples).  Experiment with the learning rate `eta` and produce plots like we showed in lecture. Which value of `eta` appears to give the best-ish convergence? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: Once you've found a reasonable learning rate, experiment with the regularization strength.  Show plots of accuracy over the training process for a few different values of `lam`.  Which seems to work the best-ish and why? \n",
    "\n",
    "**Hint**: For this type of text data, you'll want to look at very small values of `lam` (like `lam=1e-3` or maybe even smaller). \n",
    "\n",
    "Report your final accuracy on the training and validation sets after you've tuned your model in **Parts D** and **E**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part F**: Finally, go back to your LogReg class and complete the `best_text_features` function to print the 10 best predictive words for each class.  Show your results here and also **briefly** explain mathematically how you arrived at them.  Do they seem to make sense given what you know about baseball and hockey? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
