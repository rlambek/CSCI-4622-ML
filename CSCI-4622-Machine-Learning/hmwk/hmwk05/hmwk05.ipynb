{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Naive Bayes, Cross-Validation, and VC Dimension \n",
    "***\n",
    "\n",
    "**Name**: Maxwell Lambek\n",
    "\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Wednesday April 18th**. Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI-4622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy).\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:42:01.964923Z",
     "start_time": "2018-04-03T14:42:01.249718Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle, gzip\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 points] Problem 1 - Naive Bayes for Tennis Prediction \n",
    "***\n",
    "\n",
    "Suppose you are trying to learn a person's decision whether to play tennis or not on a given day using features corresponding to precipitation forecast, temperature, humidity, and wind. You're given the following training data: \n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c|c|c|c}\n",
    "\\textbf{Forecast} & \\textbf{Temp} & \\textbf{Humidity} & \\textbf{Wind} & \\textbf{PlayTennis} \\\\\n",
    "\\hline \n",
    "\\textrm{sunny} & \\textrm{hot} & \\textrm{high} & \\textrm{weak} & \\textbf{No} \\\\ \n",
    "\\textrm{sunny} & \\textrm{hot} & \\textrm{high} & \\textrm{strong} & \\textbf{No} \\\\ \n",
    "\\textrm{overcast} & \\textrm{hot} & \\textrm{high} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{mild} & \\textrm{high} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{cool} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{cool} & \\textrm{normal} & \\textrm{strong} & \\textbf{No} \\\\ \n",
    "\\textrm{overcast} & \\textrm{cool} & \\textrm{normal} & \\textrm{strong} & \\textbf{Yes} \\\\ \n",
    "\\textrm{sunny} & \\textrm{mild} & \\textrm{high} & \\textrm{weak} & \\textbf{No} \\\\ \n",
    "\\textrm{sunny} & \\textrm{cool} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{mild} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{sunny} & \\textrm{mild} & \\textrm{normal} & \\textrm{strong} & \\textbf{Yes} \\\\ \n",
    "\\textrm{overcast} & \\textrm{mild} & \\textrm{high} & \\textrm{strong} & \\textbf{Yes} \\\\ \n",
    "\\textrm{overcast} & \\textrm{hot} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{mild} & \\textrm{high} & \\textrm{strong} & \\textbf{No} \\\\ \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "**Part A**: Estimate the priors $p(\\textrm{PlayTennis=Yes})$ and $p(\\textrm{PlayTennis=No})$ from the training data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "$p$(PlayTennis=Yes) = $\\frac{9}{14}$\n",
    "\n",
    "$p$(PlayTennis=No) = $\\frac{5}{14}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: For each feature\n",
    "- state the vocabulary $V$ for the feature \n",
    "- estimate the class-conditional probabilities $p(\\textrm{feature value} \\mid \\textrm{PlayTennis})$ from the training data using Laplace add-one smoothing. Show your work. \n",
    "\n",
    "**Note**: There is no need to include any `UNK` features for this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For the Forecast feature V = \\{sunny, overcast, rainy\\}\n",
    "\n",
    "For the Temp feature V = \\{hot, mild, cold\\}\n",
    "\n",
    "For the Humidity feature V = \\{high, normal\\}\n",
    "\n",
    "For the Wind feature V = \\{weak, strong\\}\n",
    "\n",
    "p(Forecast = sunny | yes) = $\\frac{2 + 1}{9 + 3}= \\frac{3}{12} $, p(Forecast = sunny | no) = $\\frac{3 + 1}{5 + 3} = \\frac{4}{8}$,\n",
    "\n",
    "p(Forecast = overcast | yes) = $\\frac{4 + 1}{9 + 3} = \\frac{5}{12} $, p(Forecast = overcast | no) = $\\frac{0 + 1}{5 + 3} = \\frac{1}{8} $,\n",
    "\n",
    "p(Forecast = rainy | yes) = $\\frac{3 + 1}{9 + 3} = \\frac{4}{12} $, p(Forecast = rainy | no) = $\\frac{2 + 1}{5 + 3} = \\frac{3}{8}$,\n",
    "\n",
    "\n",
    "p(Temp = hot | yes) = $\\frac{2 + 1}{9 + 3} = \\frac{3}{12} $, p(Temp = hot | no) = $\\frac{2 + 1}{5 + 3} = \\frac{3}{8} $,\n",
    "\n",
    "p(Temp = mild | yes) = $\\frac{4 + 1}{9 + 3} = \\frac{5}{12} $, p(Temp = mild | no) = $\\frac{2 + 1}{5 + 3} = \\frac{3}{8} $,\n",
    "\n",
    "p(Temp = cold | yes) = $\\frac{3 + 1}{9 + 3}= \\frac{4}{12} $, p(Temp = cold | no) = $\\frac{1 + 1}{5 + 3} = \\frac{2}{8} $,\n",
    "\n",
    "\n",
    "p(Humidity = high | yes) = $\\frac{3 + 1}{9 + 2} = \\frac{4}{11} $, p(Humidity = high | no) = $\\frac{4 + 1}{5 + 2} = \\frac{5}{7} $,\n",
    "\n",
    "p(Humidity = normal | yes) = $\\frac{6 + 1}{9 + 2} = \\frac{7}{11} $, p(Humidity = normal | no) = $\\frac{1 + 1}{5 + 2} = \\frac{2}{7} $,\n",
    "\n",
    "p(Wind = strong | yes) = $\\frac{3 + 1}{9 + 2} = \\frac{4}{11} $, p(Wind = strong | no) = $\\frac{3 + 1}{5 + 2} = \\frac{4}{7} $,\n",
    "\n",
    "p(Wind = weak | yes) = $\\frac{6 + 1}{9 + 2} =\\frac{7}{11} $, p(Wind = weak | no) = $\\frac{2 + 1}{5 + 2} = \\frac{3}{7} $,\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: What would your Naive Bayes model predict for the following two weather conditions. Show all work.  \n",
    "- **Forecast**=overcast, **Temp**=cool, **Humidity**=high, **Wind**=weak  \n",
    "- **Forecast**=sunny, **Temp**=hot, **Humidity**=normal, **Wind**=strong  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$p(no|x)= p(x|no)p(no) \\approx \\hat{p}(overcast|no)*\\hat{p}(cool|no)*\\hat{p}(high|no)\\hat{p}(weak|no)* \\hat{p}(no) = \\frac{1}{8} \\frac{2}{8} \\frac{5}{7} \\frac{3}{7} \\frac{5}{14} = 0.0034165 $\n",
    "\n",
    "$p(yes|x)= p(x|yes)p(yes) \\approx \\hat{p}(overcast|yes)*\\hat{p}(cool|yes)*\\hat{p}(high|yes)\\hat{p}(weak|yes) * \\hat{p}(yes)= \\frac{5}{12} \\frac{4}{12} \\frac{4}{11} \\frac{7}{11} \\frac{9}{14} = 0.02066 $\n",
    "\n",
    "Since $p(yes|x)$ is higher the naive bayes model would predict Yes.\n",
    "\n",
    "$p(no|x)= p(x|no)p(no) \\approx \\hat{p}(sunny|no)*\\hat{p}(hot|no)*\\hat{p}(normal|no)\\hat{p}(strong|no)* \\hat{p}(no) = \\frac{4}{8} \\frac{3}{8} \\frac{2}{7} \\frac{4}{7} \\frac{5}{14} = 0.0109 $\n",
    "\n",
    "\n",
    "$p(yes|x)= p(x|yes)p(yes) \\approx \\hat{p}(sunny|yes)*\\hat{p}(hot|yes)*\\hat{p}(normal|yes)\\hat{p}(strong|yes) \\hat{p}(yes) =\\frac{3}{12} \\frac{3}{12} \\frac{7}{11} \\frac{4}{11} \\frac{9}{14} = 0.0093 $\n",
    "\n",
    "Since $p(no|x)$ is higher the naive bayes model would predict No.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [35 points] Problem 2 - Implementing Discrete Naive Bayes for Text Classification \n",
    "***\n",
    "\n",
    "In this problem you'll implement a general Discrete Naive Bayes class for text classification. Your tasks will be to implement `train`, `predict_log_score`, and `predict` routines to learn the Naive Bayes model parameters from a collection on text and make predictions on unseen validation data. \n",
    "\n",
    "The skeleton for the `TextNB` class is below. Note that this class is fairly similar to the one you worked with in the Hands-On Naive Bayes in-class notebook, so you should look there to remind yourself of the details. Scroll down to find more information about your tasks as well as unit tests.\n",
    "\n",
    "**Important Note**: In Problem 3 we'll be using the `TextNB` class to make predictions about Twitter data.  Since real-world text data typically has a large number of features, you'll want to make your implementation reasonably efficient so that your experiments in Problem 3 don't take forever. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:42:09.159492Z",
     "start_time": "2018-04-03T14:42:09.076713Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TextNB:\n",
    "    def __init__(self, text_train, y_train, alpha=1.0):\n",
    "        \"\"\"\n",
    "        :param text_train: a list or ndarray of text strings to use as training data \n",
    "        :param y_train: an ndarray of true labels associated with the text data \n",
    "        :param alpha: the Laplace smoothing parameter \n",
    "        \"\"\"\n",
    "        \n",
    "        # store training data \n",
    "        self.text_train = text_train \n",
    "        self.y_train = y_train \n",
    "        \n",
    "        # store smoothing parameter\n",
    "        self.alpha = alpha \n",
    "        \n",
    "        # get number of classes \n",
    "        self.num_classes = len(set(y_train))\n",
    "        \n",
    "        # initialize vocab to feature map \n",
    "        self.vocab = dict() \n",
    "        \n",
    "        # initialize class counts \n",
    "        self.class_counts = np.zeros(self.num_classes, dtype=int)\n",
    "        \n",
    "        # initialize feature counts (Note, will need to update this with the correct\n",
    "        # number of columns during the training process)\n",
    "        self.feature_counts = np.zeros((self.num_classes, 0), dtype=int)\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Learn the vocabularly, class_counts, and feature counts from the training data \n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        loc = 0\n",
    "        for ii in range(len(self.text_train)):\n",
    "            s = self.text_train[ii].split()\n",
    "            for jj in range(len(s)):\n",
    "                try:\n",
    "                    self.vocab[s[jj]]\n",
    "                except KeyError:\n",
    "                    self.vocab[s[jj]] = loc\n",
    "                    loc = loc + 1\n",
    "                \n",
    "        self.class_counts[0] = np.sum(self.y_train == 0)\n",
    "        self.class_counts[1] = np.sum(self.y_train == 1)\n",
    "                    \n",
    "                    \n",
    "                \n",
    "        # initialize feature counts \n",
    "        self.feature_counts = np.zeros((self.num_classes, len(self.vocab)), dtype=int)\n",
    "\n",
    "        for ii in range(len(self.text_train)):\n",
    "            s = self.text_train[ii].split()\n",
    "            for jj in range(len(s)):\n",
    "                self.feature_counts[self.y_train[ii],self.vocab[s[jj]]] = self.feature_counts[self.y_train[ii],self.vocab[s[jj]]] + 1\n",
    "                \n",
    "    def predict_log_score(self, text_str):\n",
    "        \"\"\"\n",
    "        Get the log-probability score for each class\n",
    "        for a query string\n",
    "        \n",
    "        :param text_str: a single string of text to compute the log_score for \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        s = text_str.split()\n",
    "        p0 = 0.0\n",
    "        p1 = 0.0\n",
    "        for ii in range(len(s)):\n",
    "            try:\n",
    "                up0 = self.feature_counts[0,self.vocab[s[ii]]] + self.alpha\n",
    "                low0 = np.sum(self.feature_counts[0,:])+len(self.vocab)*self.alpha\n",
    "                p0 = p0 + np.log(up0/low0)\n",
    "\n",
    "                up1 = self.feature_counts[1,self.vocab[s[ii]]] + self.alpha\n",
    "                low1 = np.sum(self.feature_counts[1,:])+len(self.vocab)*self.alpha\n",
    "                p1 = p1 + np.log(up1/low1)\n",
    "            except KeyError:\n",
    "                pass\n",
    "                #print(\"ignored,\", s[ii])\n",
    "        pclass0 = np.log((self.class_counts[0] + self.alpha)/(self.class_counts[0] + self.class_counts[1] + 2*self.alpha))\n",
    "        pclass1 = np.log((self.class_counts[1] + self.alpha)/(self.class_counts[0] + self.class_counts[1] + 2*self.alpha))\n",
    "        \n",
    "        class_scores = np.zeros(self.num_classes) \n",
    "        class_scores[0] = p0+pclass0\n",
    "        class_scores[1] = p1 + pclass1\n",
    "        \n",
    "        \n",
    "        return class_scores\n",
    "        \n",
    "    \n",
    "    def predict(self, text_list):\n",
    "        \"\"\"\n",
    "        Predict the class of each example in text_list  \n",
    "        \n",
    "        :param text_list: a list or ndarray of text strings to make predictions on \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO \n",
    "        \n",
    "        yhat = np.zeros(len(text_list), dtype=int)\n",
    "        for ii in range(len(text_list)):\n",
    "            yhat[ii] = np.argmax(self.predict_log_score(text_list[ii]))\n",
    "        return yhat \n",
    "        \n",
    "        \n",
    "    def accuracy(self, text_list, y_true):\n",
    "        \"\"\"\n",
    "        Make predictions on texts in text_list and compute accuracy relative to \n",
    "        true labels in y_true \n",
    "        \n",
    "        :param text_list: a list or ndarray of text strings to make predictions on \n",
    "        :param y_list: an ndarray of true labels associated with the text data \n",
    "        \"\"\"\n",
    "        yhat = self.predict(text_list)\n",
    "        return np.sum(yhat == y_true)/len(y_true)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Complete the `train` function in the `TextNB` class to prepare to make Naive Bayes predictions using Laplace smoothing.  In this routine you will need to populate the following data structures: \n",
    "\n",
    "`self.vocab`: A Python dictionary that maps distinct terms found in the training set to unique indices in $\\{0, 1, \\ldots, |V|-1\\}$.  This will allow us to quickly look up frequency counts for an encountered term in a Numpy array. Note that while the data is fairly clean (We've removed punctuation and made all characters lowercase) you should take care that you're not accidentally counting whitespace in the vocabulary. \n",
    "\n",
    "`self.class_counts`: A 1D Numpy array of length `self.num_classes` which counts the number of documents in the training set that belong to each class. \n",
    "\n",
    "`self.feature_counts`: A 2D Numpy array of dimensions `self.num_classes` $\\times$ $|V|$. The $(c,k)$-entry in this array should be the number of times that term $k$ appears in documents belonging to class $c$. Note that we're using the Bag-of-Words approach here, so if a term appears multiple times in a single document, each instance of that term should be counted.  \n",
    "\n",
    "When you think you're done, execute the following unit test, corresponding to the example starting on Slide 29 of [Lecture 24](https://www.cs.colorado.edu/~ketelsen/files/courses/csci4622/slides/lesson24.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:45:46.325834Z",
     "start_time": "2018-04-03T14:45:46.314267Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testVocab (__main__.TestNB) ... ok\n",
      "testClassCounts (__main__.TestNB) ... ok\n",
      "testFeatureCounts (__main__.TestNB) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.012s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py \"prob 2A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Complete the `predict_log_score` function in the `TextNB` class to take in a single string of text and compute the associated log-score for each class. For now, you should use add-one Laplace smoothing for both the class-priors and the class-conditional probabilities.  In **Problem 3** we'll experiment with different variants of Laplace smoothing, so if you like you can read ahead now and implement the general version of Laplace smoothing from the beginning.  \n",
    "\n",
    "**Note**: For simplicity and testing purposes, do not implement an `UNK` feature.  Instead, if you encounter a term not in the vocabulary you can safely ignore it. \n",
    "\n",
    "When you think your `predict_log_score` function is working well, execute the following unit test. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:45:47.998683Z",
     "start_time": "2018-04-03T14:45:47.990764Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testLogScore (__main__.TestNB) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.006s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py \"prob 2B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Finally, implement the `predict` method to take in a list or ndarray of text data, call `predict_log_score`, and return a vector of predicted labels. \n",
    "\n",
    "When you think you're done, execute the following unit test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:45:49.228361Z",
     "start_time": "2018-04-03T14:45:49.220931Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testPredict (__main__.TestNB) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.009s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py \"prob 2C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [30 points] Problem 3: Predicting the Sentiment of Tweets sent from Passengers to Airlines \n",
    "***\n",
    "\n",
    "In this problem you'll use the `TextNB` class you wrote in **Problem 2** to make predictions about the sentiment of tweets sent by passengers to airlines.  Execute the following cell to load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T14:45:55.395606Z",
     "start_time": "2018-04-03T14:45:55.371833Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('data/airline_tweets.pklz','rb')\n",
    "text_train, y_train, text_valid, y_valid, text_all, y_all = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Explore the data and answer the following questions: \n",
    "\n",
    "- How many total examples are there in the training and validation sets? \n",
    "- Which binary label ($\\{0,1\\}$) corresponds to tweets with positive and negative sentiment, respectively?\n",
    "- What percentage of tweets in the training set have true positive sentiment? \n",
    "- What percentage of tweets in the validation set have true positive sentiment? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "1000\n",
      "3077\n",
      "923\n",
      "[1 0 0 ..., 0 0 0]\n",
      "['@southwestair awesome  thanks'\n",
      " '@usairways good news weve located the crew and made contact with them flight was supposed to leave 4 minutes ago usairwaysfail'\n",
      " '@usairways 45 minute delay for take off and 30 minute wait for checked bags really'\n",
      " ...,\n",
      " '@southwestair will continue to be my airline of choice @united most frustrating travel day ive experienced 5 delays and 4 gate changes'\n",
      " '@americanair would it be ok to send you a dm asking a few questions because im and been on hold so long'\n",
      " '@united this is it last time i fly unitedairlines  you screw up every trip now will be stuck in ord and miss work']\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(y_train == 0))\n",
    "print(np.sum(y_train == 1))\n",
    "\n",
    "print(np.sum(y_valid == 0))\n",
    "print(np.sum(y_valid == 1))\n",
    "print(y_train)\n",
    "print(text_train)#1 is positive, and 0 is negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4000 examples in the training set, and 4000 examples in the validation set.\n",
    "\n",
    "The binary label 0 corresponds to negative sentiment, which can be seen very clearly in the third training example. The binary label 1 corresponds to positive sentiment, as seen in the first training example.\n",
    "\n",
    "$\\frac{1000}{4000} = 25\\%$ of the tweets in the training set have true positive sentiment.\n",
    "\n",
    "$\\frac{923}{4000} = 23.075\\%$ of the tweets in the validation set have true positive sentiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Use your `TextNB` class to learn a Naive Bayes classifier for the airline Twitter data.  What accuracy do you achieve on the training set and what accuracy do you achieve on the test set? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = TextNB(text_train, y_train)\n",
    "nb.train()\n",
    "acc_train = nb.accuracy(text_train, y_train)\n",
    "acc_valid = nb.accuracy(text_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9305 0.91075\n"
     ]
    }
   ],
   "source": [
    "print(acc_train,acc_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Notice that if we want to make improvements in our Naive Bayes classifier, we don't really have a lot of knobs to turn aside from changing the word features that we use.  One place that we might make some gains though is in using a slightly different version of Laplace smoothing.  \n",
    "\n",
    "Recall that in add-one smoothing we add a $1$ to the numerator in both the estimation of the prior probabilities and the class-conditional likelihoods. \n",
    "\n",
    "$$\n",
    "\\hat{p}(\\textrm{Class}) = \\dfrac{\\textrm{# docs from Class}+1}{\\textrm{# total docs in training data} + |C|},\n",
    "\\quad \\quad\n",
    "\\hat{p}(\\textrm{term} \\mid \\textrm{Class}) = \\dfrac{\\textrm{# instance of term in Class}+1}{\\textrm{# total words in Class} + |V|}\n",
    "$$\n",
    "\n",
    "It turns out there's nothing sacred about adding $1$ to the numerators.  In fact, we can add any positive value $\\alpha$ that we like \n",
    "\n",
    "$$\n",
    "\\hat{p}(\\textrm{Class}) = \\dfrac{\\textrm{# docs from Class}+\\alpha}{\\textrm{# total docs in training data} + ?},\n",
    "\\quad \\quad\n",
    "\\hat{p}(\\textrm{term} \\mid \\textrm{Class}) = \\dfrac{\\textrm{# instance of term in Class}+\\alpha}{\\textrm{# total words in Class} + ?}\n",
    "$$\n",
    "\n",
    "Explain what modification must be made to the denominators so that theses estimates remain valid probabilities. Clearly justify your reasoning. \n",
    "\n",
    "Support this modification in your `TextNB` class above, if you have not already.  Make sure that your code still passes then unit tests when $\\alpha = 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modification that needs to happen is we need to add $\\alpha * |C|$ to the denominator for $\\hat{p}(Class)$ and $\\alpha * |V|$ for  $\\hat{p}(term|Class)$. This is because we are adding $\\alpha$ to each class or term. So, in order to remain as a probability we need to divide by the total amount of $\\alpha$'s we add, which is $\\alpha * |C|$ or $\\alpha * |V|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9365 0.914\n"
     ]
    }
   ],
   "source": [
    "nb = TextNB(text_train, y_train,alpha=0.5)\n",
    "nb.train()\n",
    "acc_train = nb.accuracy(text_train, y_train)\n",
    "acc_valid = nb.accuracy(text_valid, y_valid)\n",
    "print(acc_train,acc_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Write some code to perform $K$-Folds cross-validation on the entire data set (`train_all` and `y_all`) to estimate the accuracy of your NB classifier for various values of $\\alpha$ and make a plot showing your results.  \n",
    "\n",
    "To do the partitioning into folds I recommend leveraging sklearn's [StratifiedKFold]() routine.  The documentation demonstrates how it can be used.  \n",
    "\n",
    "For your plot, use at least $K=5$ folds and at least $5$ different values of $\\alpha$ between $0.1$ and $1.5$.  Which value of $\\alpha$ seems to perform the best? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "k = 5\n",
    "skf = StratifiedKFold(n_splits=k)\n",
    "acc_train = []\n",
    "acc_valid = []\n",
    "#skf.get_n_splits(text_all, y_all)\n",
    "for train_index, test_index in skf.split(text_all, y_all):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    Xt, Xv = text_all[train_index], text_all[test_index]\n",
    "    yt, yv = y_all[train_index], y_all[test_index]\n",
    "    nb = TextNB(Xt, yt,alpha=0.8)\n",
    "    nb.train()\n",
    "    acc_train =acc_train + [nb.accuracy(Xt, yt)]\n",
    "    acc_valid = acc_valid + [nb.accuracy(Xv, yv)]\n",
    "    #print(acc_train,acc_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.91692692067457837, 0.89444097439100567, 0.91437500000000005, 0.91619762351469669, 0.9130706691682301]\n"
     ]
    }
   ],
   "source": [
    "print(acc_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.910378018605\n",
      "0.911377823658\n",
      "0.91104445114\n",
      "0.910940069642\n",
      "0.910952503224\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "k = 5\n",
    "skf = StratifiedKFold(n_splits=k)\n",
    "acc_train = []\n",
    "acc_valid = []\n",
    "#skf.get_n_splits(text_all, y_all)\n",
    "for train_index, test_index in skf.split(text_all, y_all):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    Xt, Xv = text_all[train_index], text_all[test_index]\n",
    "    yt, yv = y_all[train_index], y_all[test_index]\n",
    "    nb = TextNB(Xt, yt,alpha=1)\n",
    "    nb.train()\n",
    "    acc_train =acc_train + [nb.accuracy(Xt, yt)]\n",
    "    acc_valid = acc_valid + [nb.accuracy(Xv, yv)]\n",
    "    #print(acc_train,acc_valid)\n",
    "alpha1CV = np.mean(acc_valid)\n",
    "print(alpha1CV)\n",
    "for train_index, test_index in skf.split(text_all, y_all):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    Xt, Xv = text_all[train_index], text_all[test_index]\n",
    "    yt, yv = y_all[train_index], y_all[test_index]\n",
    "    nb = TextNB(Xt, yt,alpha=0.5)\n",
    "    nb.train()\n",
    "    acc_train =acc_train + [nb.accuracy(Xt, yt)]\n",
    "    acc_valid = acc_valid + [nb.accuracy(Xv, yv)]\n",
    "    #print(acc_train,acc_valid)\n",
    "alpha05CV = np.mean(acc_valid)\n",
    "print(alpha05CV)\n",
    "\n",
    "for train_index, test_index in skf.split(text_all, y_all):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    Xt, Xv = text_all[train_index], text_all[test_index]\n",
    "    yt, yv = y_all[train_index], y_all[test_index]\n",
    "    nb = TextNB(Xt, yt,alpha=0.1)\n",
    "    nb.train()\n",
    "    acc_train =acc_train + [nb.accuracy(Xt, yt)]\n",
    "    acc_valid = acc_valid + [nb.accuracy(Xv, yv)]\n",
    "    #print(acc_train,acc_valid)\n",
    "alpha01CV = np.mean(acc_valid)\n",
    "print(alpha01CV)\n",
    "\n",
    "for train_index, test_index in skf.split(text_all, y_all):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    Xt, Xv = text_all[train_index], text_all[test_index]\n",
    "    yt, yv = y_all[train_index], y_all[test_index]\n",
    "    nb = TextNB(Xt, yt,alpha=1.5)\n",
    "    nb.train()\n",
    "    acc_train =acc_train + [nb.accuracy(Xt, yt)]\n",
    "    acc_valid = acc_valid + [nb.accuracy(Xv, yv)]\n",
    "    #print(acc_train,acc_valid)\n",
    "alpha15CV = np.mean(acc_valid)\n",
    "print(alpha15CV)\n",
    "\n",
    "\n",
    "for train_index, test_index in skf.split(text_all, y_all):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    Xt, Xv = text_all[train_index], text_all[test_index]\n",
    "    yt, yv = y_all[train_index], y_all[test_index]\n",
    "    nb = TextNB(Xt, yt,alpha=0.8)\n",
    "    nb.train()\n",
    "    acc_train =acc_train + [nb.accuracy(Xt, yt)]\n",
    "    acc_valid = acc_valid + [nb.accuracy(Xv, yv)]\n",
    "    #print(acc_train,acc_valid)\n",
    "alpha08CV = np.mean(acc_valid)\n",
    "print(alpha08CV)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.913709087403\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in skf.split(text_all, y_all):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    Xt, Xv = text_all[train_index], text_all[test_index]\n",
    "    yt, yv = y_all[train_index], y_all[test_index]\n",
    "    nb = TextNB(text_train, y_train,alpha=0.2)\n",
    "    nb.train()\n",
    "    acc_train =acc_train + [nb.accuracy(Xt, yt)]\n",
    "    acc_valid = acc_valid + [nb.accuracy(Xv, yv)]\n",
    "    #print(acc_train,acc_valid)\n",
    "alpha02CV = np.mean(acc_valid)\n",
    "print(alpha02CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuwAAAF8CAYAAACUru4iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIgkMBXsIiIiIiIJ7P8BI5ppqerceZUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fecbc37d780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ep = [0.1,0.2,0.5,0.8,1,1.5] \n",
    "yax = [alpha01CV,alpha02CV, alpha05CV, alpha08CV, alpha1CV, alpha15CV]\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "ax.plot(ep,yax, color = \"green\")\n",
    "ax.set_xlabel(\"alpha\", fontsize=16)\n",
    "ax.set_ylabel(\"CV accuracy\", fontsize=16);\n",
    "ax.grid(alpha=0.25)\n",
    "ax.legend(loc=\"lower right\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha = 0.2$ seems to preform the best. The optimal $\\alpha$ value is probably not $0.2$ but it is close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Problem 4: VC Dimension \n",
    "***\n",
    "\n",
    "**Part A**: Consider learning to classify binary labeled data with a single feature $x$.  Let $H$ be the hypothesis class described by the union of two intervals $[a,b] \\cup [c,d]$ such that $h(x)$ labels an example as positive if it's in the interval $[a,b]$ **OR** the interval $[c,d]$.  Determine the VC Dimension of $H$.  Justify your conclusion by demonstrating a shattering of a set $S$ of the appropriate size **AND** by arguing that an arbitrary set consisting of one additional point cannot be shattered by $H$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The VC Dimension of H is 4.\n",
    "\n",
    "Step1: show for 4 points. Let 0 be negative and 1 be positive and [] represent an inerval [a,b] or [c,d]\n",
    "\n",
    "[1111] [] $\\hspace{1cm}$ [111]0 [] $\\hspace{1cm}$ [11]0[1] $\\hspace{1cm}$ [1]0[11] $\\hspace{1cm}$ []0[111]\n",
    "\n",
    "[11]00[] $\\hspace{1cm}$ [1]0[1]0 $\\hspace{1cm}$ 0[11]0[] $\\hspace{1cm}$ 0[1]0[1] $\\hspace{1cm}$ []00[11]\n",
    "\n",
    "[1]000[] $\\hspace{1cm}$ 0[1]00 $\\hspace{1cm}$ 00[1]0[] $\\hspace{1cm}$ 000[1] $\\hspace{1cm}$ []0000[]\n",
    "\n",
    "Now, since all sets S such that $|S| = 4$ have been correctly classified the set $S$ has been shattered $ \\implies VCdim(H) \\geq 4$.\n",
    "\n",
    "Step2: show it cannot be done for 5 points.\n",
    "\n",
    "[1]0[1]01 but then 1 point is missclassifed. [1]010[1] again, 1 point missclassified. 10[1]0[1], [101]0[1], [1]0[101]. So, as you can see no matter how I place the two intervals $[a,b]$ and $[c,d]$ one point or more is missclassifed, and therefore $4 \\leq VCdim(H) < 5 \\implies VCdim(H)  = 4$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Consider learning to classify binary labeled data with two features, $x_1$ and $x_2$.  Let $H$ be the hypothesis class described by the ability to assign all points in a particular quadrant of the 2D plane to be positive or negative, with the restriction that at least one of the four quadrants must be labeled positive and at least one of the four quadrants must be labeled negative. Determine the VC Dimension of $H$. Again, justify your conclusion by demonstrating a shattering of a set $S$ of the appropriate size **AND** by arguing that an arbitrary set consisting of one additional point cannot be shattered by $H$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The $VCdim(H) = 4$.\n",
    "\n",
    "Step1: show for 4 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-201-ffd35b229113>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-201-ffd35b229113>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    |\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        |1 1\n",
    "     ___|___    #The first quadrant is negative and all others are positive\n",
    "      1 | 1\n",
    "        |\n",
    "        \n",
    "     1  | 0\n",
    "     ___|___   #The 2nd quadrant is neagtive and all others are positive. All other placements of the 0\n",
    "      1 | 1    # can be handeled by rotating the axis to get back to this situation.\n",
    "        |\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "      1 | 0\n",
    "     ___|___ #The 4th quadrant and 2nd quadrant is negative and all others are positive\n",
    "      1 | 0  # rotations cover all other setups like this\n",
    "        |\n",
    "        \n",
    "        \n",
    "    1   | 0\n",
    "     ___|___  #The 4th quadrant is positive and all others are negative\n",
    "      0 | 1  # again, rotations cover the other setups with this structure\n",
    "        |\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    1   | 0\n",
    "     ___|___ #The 1st quadrant is positive and all others are negative\n",
    "      0 | 0  #rotations again\n",
    "        |\n",
    "      \n",
    "        |0 0\n",
    "     ___|___ #The 1st quadrant is positive and all others are negative\n",
    "      0 | 0\n",
    "        |\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since all sets S such that $|S| = 4$ have been correctly classified the set $S$ has been shattered $ \\implies VCdim(H) \\geq 4$.\n",
    "\n",
    "Step2: showing this cannot be done for 5 points,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "      1 |0 1  #no matter what quadrant I assign to be negative there will be a positive that is\n",
    "     ___|___  # missclassified. Also, rotations on the axis/points wont work either because the\n",
    "      1 | 1   # top row cannot be seperated.\n",
    "        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, $4 \\leq VCdim(H) <5 \\implies VCdim(H) = 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
