{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Regression, Regularization, and the Bias-Variance Trade-Off\n",
    "***\n",
    "\n",
    "**Name**: \n",
    "\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday February 2nd**. Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI-4622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy).\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Problem 1 - Getting Comfortable with MathJax/LaTeX\n",
    "***\n",
    "\n",
    "Most homeworks in this course will require you to write solutions to at least one or two math-based exercises.  This problem is designed to motivate you to learn some MathJax-LaTeX for typesetting math in Jupyter Notebooks.  [LaTeX](https://en.wikipedia.org/wiki/LaTeX) is a markup language used for typesetting mathematical formulas and documents.  [MathJax](https://en.wikipedia.org/wiki/MathJax) is a LaTeX plug-in for Markdown that brings some LaTeX functionality to Jupyter. A good tutorial on MathJax can be found in this [StackExchange post](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference/5024). \n",
    "\n",
    "Your goal is this problem is to use MathJax to reproduce a collection of formulas as accurately as possible. Note that because computing environments vary from person to person, you shouldn't worry about tiny details like matching fonts (though you should worry about normal vs italic vs bold).  Just do your best to get reasonably close. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A** Reproduce the expression shown below using MathJax: \n",
    "\n",
    "<img src=\"figs/prob1A.png\" width=\"300\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B** Reproduce the expression shown below using MathJax: \n",
    "\n",
    "<img src=\"figs/prob1B.png\" width=\"180\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C** Reproduce the expression shown below using MathJax: \n",
    "\n",
    "<img src=\"figs/prob1C.png\" width=\"250\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [30 points] Problem 2 - Building a Data Storage Class \n",
    "***\n",
    "\n",
    "In this exercise you will get some practice constructing classes in Python.  If you are rusty on object-oriented Python, you should check out [this tutorial](https://www.digitalocean.com/community/tutorials/how-to-construct-classes-and-define-objects-in-python-3).  Our goal will be to create a class which takes in a set of labeled data, randomly splits it into training and validation sets, and then stores these sets for later use. You will also implement the ability to mean-center and standardize features, and apply these same transformations to new test data after the fact.  A starting point for RegressionData class appears below.  \n",
    "\n",
    "**Notes**: \n",
    "- This problem will be graded by unit test, so do not change function or method APIs, but feel free to create any additional variables or methods that you think will be helpful. We've given you access to versions of those unit tests down below, so you'll be able to check your work as you go.  \n",
    "- Do not use any additional functions from sklearn.  (There is an sklearn function called train_test_split that does something similar.  You'll be allowed to use it later on.  This exercise is about getting your hands dirty so you can understand how things work under the hood)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegressionData:\n",
    "    \"\"\"\n",
    "    Class to store data for regression problems \n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, train_frac=0.8, center=False, standardize=False, random_state=1234):\n",
    "        \"\"\"\n",
    "        Creates a RegressionData instance\n",
    "\n",
    "        :param X: (n x p) ndarray of feature data \n",
    "        :param y: (n x 1) ndarray of labels/targets  \n",
    "        :param train_frac: float indicating fraction of data to train on \n",
    "        :param center: bool indicating whether to mean-center the features \n",
    "        :param standardize: bool indicating whether to mean_center and standardize \n",
    "        :param random_state: integer seed for random number generators\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set random seed (for testing purposes, don't change this line) \n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # These should probably be set, eventually\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_valid = None\n",
    "        self.y_valid = None\n",
    "        \n",
    "        # Perform train-validation split \n",
    "        self.train_valid_split(X, y, train_frac)\n",
    "            \n",
    "        # standardize and/or center the data if requested\n",
    "        self.transform_train_valid(center, standardize)\n",
    "        \n",
    "    def train_valid_split(self, X, y, train_frac):\n",
    "        \"\"\"\n",
    "        Randomly splits the data into training and validation sets \n",
    "\n",
    "        :param X: (n x p) ndarray of feature data \n",
    "        :param y: (n x 1) ndarray of labels/targets  \n",
    "        :param train_frac: float indicating fraction of data to train on \n",
    "        \"\"\"\n",
    "        return\n",
    "        \n",
    "    def transform_train_valid(self, center, standardize):\n",
    "        \"\"\"\n",
    "        Standardizes and/or centers train and validation sets, if requested\n",
    "\n",
    "        :param center: bool indicating whether to mean-center the features \n",
    "        :param standardize: bool indicating whether to mean_center and standardize \n",
    "        \"\"\"\n",
    "        return\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Applies transformations performed on training set to \n",
    "        previously unseen data x \n",
    "\n",
    "        :param X: (m x p) ndarray of feature data \n",
    "        \"\"\"\n",
    "        Xt = X.copy()\n",
    "        return Xt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Complete the method `train_valid_split` to randomly initialize the train and validation data according to the split proportion indicated by `train_frac`. When you think you're done, execute the unit tests at the end of this problem.  If you're successful, you should pass the first 2 tests.  Don't move on until you've passed them! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Complete the method `transform_train_valid` function to standardize and/or center the features depending on the values of `center` and `standardize`.  Remember that if both flags are set, each column in the **training data** should be shifted to have mean zero, and then scaled to have standard deviation of 1. Each column of the validation data should then undergo the same shift and scale **based on the training data**.  When you think you're done, execute the unit tests again. If you're successful, you should pass four more unit tests.  Don't move on until you do! \n",
    "\n",
    "**Notes**: \n",
    "\n",
    "- How you implement this method might affect **Part C**.  It's a good idea to read **Part C** now so that you can plan ahead. \n",
    "- It's rare in a regression setting that you would ever standardize a feature without centering it first, so we will not be testing this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Suppose that later on in your pipeline you collect a new set of data that you would like to test your model on.  Since your model was probably constructed using the transformed features of your training set, you have to apply that same transformation to your new data.  Complete the `transform` function above so that it takes in an ndarray of newly acquired data and returns the appropriately transformed version. When you think you're done, execute the unit tests one more time.  If you're successful, you should pass the final test, at which point you will be done with this problem forever (sorta).   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute this cell to run unit tests \n",
    "%run -i tests/tests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [30 points] Problem 3 - Solving Ridge Regression\n",
    "***\n",
    "\n",
    "In this problem you'll derive a closed form solution for single-feature Ridge Regression and use it to interpret the behavior for different values of the regularization parameter $\\lambda$. Recall that for a model of the form $Y = \\beta_0 + \\beta_1 X + \\epsilon$ and a training set $\\{(x_i, y_i)\\}_{i=1}^n$, the parameter estimates are obtained by minimizing the penalized RSS: \n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta}}{\\textrm{argmin}} ~\\textrm{RSS}_\\lambda =  \\underset{\\boldsymbol{\\beta}}{\\textrm{argmin}} \\sum_{i=1}^n \\left( \\beta_0 + \\beta_1 x_i - y_i\\right)^2 + \\lambda \\beta_1^2\n",
    "$$\n",
    "\n",
    "In Calc 1 you saw that you can find the value of the parameter that minimizes a function by taking a derivative with respect to that parameter, setting it to 0, and solving. The problem here is that we have **two** parameters we need to minimize over. It turns out, for a certain class of problems, we can solve for two parameters in a similar way.  For two parameters we can take _partial_ derivatives with respect to each parameter, set equal to zero, and solve simultaneously for the optimal parameters.  That is, we want to solve the following two equations for $\\beta_0$ and $\\beta_1$: \n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial~\\textrm{RSS}_\\lambda}{\\partial \\beta_0} = 0 \n",
    "\\quad \\textrm{and} \\quad\n",
    "\\dfrac{\\partial~\\textrm{RSS}_\\lambda}{\\partial \\beta_1} = 0 \n",
    "$$\n",
    "\n",
    "Simple enough, right?  Kinda.  It turns out that even this is a bit tricky, unless you modify your data in a particular way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Suppose that we mean-center our feature $x$ by replacing each $x_i$ by $\\hat{x}_i = x_i - \\bar{x}$. Show that in this case, the Ridge Regression estimate of the bias is $\\hat{\\beta}_0 = \\bar{y}$ (the sample mean of the response $y$) by taking the partial derivative of $\\textrm{RSS}_\\lambda$ with respect to $\\beta_0$, setting it to zero, and solving for $\\beta_0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Next, solve for the Ridge Regression estimate of the slope, $\\hat{\\beta}_1$, by taking the partial derivative of $\\textrm{RSS}_\\lambda$ with respect to $\\beta_1$, setting it to zero, and solving for $\\beta_1$ as a function of the data and of $\\lambda$.  (**Hint**: You'll want to use the value of $\\hat{\\beta}_0$ you found in **Part A**). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Use the closed-form solution you found in **Part B** to _briefly_ describe the effect of the regularization parameter $\\lambda$ on the slope parameter as $\\lambda \\rightarrow 0$ and $\\lambda \\rightarrow \\infty$. Does this jive with the discussion of regularization from class? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Verify that the solution obtained in **Parts A** and **B** are correct by testing it on some simulated data and comparing the results to those obtained by sklearn's [Ridge Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) routine.  We've given you some data below, but feel free to mess with the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 2.0 \n",
    "X = np.random.uniform(size=(200,1))\n",
    "y = 2 + 3 * X[:,0] + 0.5*np.random.randn(200)\n",
    "\n",
    "# Load the data into our data structure and center it \n",
    "cdata = RegressionData(X, y, center=True)\n",
    "\n",
    "# Compute beta_0 and beta_1 using the formulas you derived above\n",
    "b0, b1 = 0, 0 \n",
    "\n",
    "# Compute beta_0 and beta_1 by calling Ridge()\n",
    "from sklearn.linear_model import Ridge \n",
    "b0, b1 = 0, 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: We claimed in the class that if you built a regularized regression model based on centered features, the model would make the same prediction as an equivalent model built with un-centered features. Verify this fact by \n",
    "\n",
    "- Predicting the response for your centered Validation set using the model you computed on your own.  \n",
    "- Predicting the response for the equivalent un-centered Validation set using sklearn's Ridge model trained on the un-centered features. \n",
    "- Showing that the predictions are the same for a reasonable subset of the Validation set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [30 points] Problem 4 - Polynomial Regression and the Bias-Variance Trade-Off\n",
    "***\n",
    "\n",
    "`#coming_soon`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
