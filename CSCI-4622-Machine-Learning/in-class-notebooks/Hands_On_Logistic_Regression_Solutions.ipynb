{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Text Models and Logistic Regression \n",
    "***\n",
    "\n",
    "In this notebook we'll explore Scikit-Learn's ability to create features from text, including how to use the Bag-of-Words model.  We'll also explore a more sophisticated feature creation technique called TFIDF.  Finally, we'll use Scikit-Learn's Logistic Regression classifier to make semantic predictions about movie reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: The Bag-of-Words Text Model \n",
    "***\n",
    "\n",
    "In the first portion of this notebook we'll explore the bag of words text model.  In lecture we worked with the following corpus of documents related to newspaper titles: \n",
    "\n",
    "$\\texttt{Training Set}:$\n",
    "\n",
    "$\\texttt{d1}: \\texttt{new york new tribune}$\n",
    "\n",
    "$\\texttt{d2}: \\texttt{new york times}$\n",
    "\n",
    "$\\texttt{d3}: \\texttt{los angeles times}$\n",
    "\n",
    "First we'll define the vocabulary based on the words in the test set.  It is $V = \\{ \\texttt{angeles}, \\texttt{los}, \\texttt{new}, \\texttt{times}, \\texttt{tribune}, \\texttt{york}\\}$. \n",
    "\n",
    "We need to define an association between the particular words in the vocabulary and the specific entries in our vectors.  Let's define this association in the order that we've listed them above.  We can store this mapping as a Python dictionary as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = {'angeles': 0, 'los': 1, 'new': 2, 'times': 3, 'tribune': 4, 'york': 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also store the documents in a list as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = [\"new york new tribune\", \"new york times\", \"los angeles times\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be consistent with sklearn conventions, we'll encode the documents as *row-vectors* stored in a **Document-Term Matrix**.  In this case, each row of the matrix corresponds to a document, and each column corresponds to a term in the vocabulary.  For our example this gives us a matrix `Xdt` of shape $3 \\times 6$.  The $(d,t)$-entry in `Xdt` is then the number of times the term $t$ appears in document $d$\n",
    "\n",
    "**Part A**: Your first task is to write some simple Python code to construct the document-term matrix `Xdt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  2.  0.  1.  1.]\n",
      " [ 0.  0.  1.  1.  0.  1.]\n",
      " [ 1.  1.  0.  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "Xdt = np.zeros((len(D),len(V)))\n",
    "\n",
    "for ii, doc in enumerate(D): \n",
    "    for term in doc.split(): \n",
    "        Xdt[ii, V[term]] += 1\n",
    "        \n",
    "print(Xdt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully your code returns the matrix \n",
    "\n",
    "$$\\texttt{Xdt} = \n",
    "\\left[\n",
    "\\begin{array}{ccccccc}\n",
    "0 & 0 & 2 & 0 & 1 & 1 \\\\\n",
    "0 & 0 & 1 & 1 & 0 & 1 \\\\\n",
    "1 & 1 & 0 & 1 & 0 & 0 \\\\\n",
    "\\end{array}\n",
    "\\right]$$.  \n",
    "\n",
    "Note that the entry in the (2,0) position is $1$ because the first word (angeles) appears once in the third document. Similarly the entry in the (0,2) position is $2$ because the third word (new) appears twice in the first document. \n",
    "\n",
    "OK, let's see how we can construct the same document-term matrix in sklearn.  We will use something called the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">CountVectorizer</a> to accomplish this. Let's see some code and then we'll explain how it functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-faf40332abdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m    \u001b[0;31m# import CountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbagofwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m                                 \u001b[0;31m# initialize the vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mXbw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbagofwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m                              \u001b[0;31m# fit to training data and transform to matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'D' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer    # import CountVectorizer \n",
    "bagofwords = CountVectorizer()                                 # initialize the vectorizer\n",
    "Xbw = bagofwords.fit_transform(D)                              # fit to training data and transform to matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\texttt{fit_transform}$ method actually does two things.  It fits the model to the training data by building a vocabulary.  It then transforms the text in $D$ into matrix form.  \n",
    "\n",
    "If we wish to see the vocabulary you can do it like so "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'new': 2, 'york': 5, 'tribune': 4, 'times': 3, 'los': 1, 'angeles': 0}\n"
     ]
    }
   ],
   "source": [
    "print(bagofwords.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is the same vocabulary and indexing that we defined ourselves.  Hopefully that means we'll get the same term-frequency matrix.  We can print `Xbw` and check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 2 0 1 1]\n",
      " [0 0 1 1 0 1]\n",
      " [1 1 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(Xbw.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, they're the same!  Notice that we had to convert $X$ to a dense matrix for printing.  This is because CountVectorizer actually returns a sparse matrix.  This is a very good thing since most vectors in a text model will be **extremely** sparse, since most documents will only contain a handful of words from the vocabulary. \n",
    "\n",
    "\n",
    "OK, now suppose that we have a query document not included in the training set that we want to vectorize.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d4 = [\"new york post\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already fit the CountVectorizer to the training set, so all we need to do is transform the test set documents into a term-frequency vector using the same conventions.  Since we've already fit the model, we do the transformation with the $\\texttt{transform}$ method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x4 = bagofwords.transform(d4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print it and see what it looks like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(x4.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: What's missing from `x4` that we might expect to see from the query document? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The word $\\texttt{post}$ does not appear in vector `x4` at all.  This is because it did not occur in the training set, which means it is not present in the vocabulary.  This should not bother us too much.  Most reasonable text data sets will have most of the important words present in the training set and thus in the vocabulary.  On the other hand, the throw-away words that are present only in the test set are probably useless anyway, since the learning model is trained based on the text in the training set, and thus won't be able to do anything intelligent with words the model hasn't seen yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Part 2: Classifying Semantics in Movie Reviews\n",
    "***\n",
    "> The data for this problem was taken from the <a href=\"https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\">Bag of Words Meets Bag of Popcorn</a> Kaggle competition\n",
    "\n",
    "In this problem you will use the text from movie reviews to predict whether the reviewer felt positively or negatively about the movie using a Bag-of-Words text model. I've partially cleaned the data and stored training and validation sets in `data/movieReviews/movieTrainData.tsv` and `data/movieReviews/movieValidationData.tsv`.  We'll load the data and process it a bit more in the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_and_clean_data(fname, remove_stops=True):\n",
    "    \n",
    "    # Read in text and labels from file \n",
    "    df = pd.read_csv(fname, sep='\\t', header=None)\n",
    "    df.columns = [\"labels\", \"text\"]\n",
    "    \n",
    "    # Make everything lowercase\n",
    "    df.loc[:,\"text\"] = df[\"text\"].apply(lambda s: s.lower())\n",
    "    \n",
    "    return df[\"text\"].values, df[\"labels\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_train, labels_train = read_and_clean_data(\"data/movieReviews/movieTrainData.tsv\")\n",
    "text_valid, labels_valid = read_and_clean_data(\"data/movieReviews/movieValidationData.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Look at a few of the reviews stored in $\\texttt{text_train}$ as well as their associated labels in $\\texttt{labels_train}$.  Can you figure out which label refers to a positive review and which refers to a negative review? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text labeled 1\n",
      "\n",
      " this film could be one of the most underrated film of bollywood history this blockbuster had all of it good performances music and direction i remember i was in allahabad when this movie was running and it was somewhere in march at holi time the people there were playing its song ooe amma at their loudspeakers in highest volume if someone who likes to watch some like it hot and drools over marilyn monroe he should see this movie thumbs up to govinda how many of you know that this film was shot in south of india and after sholay could be one of the very few blockbuter to hit silver screen with films like these indian comedy could never be dead\n"
     ]
    }
   ],
   "source": [
    "ind = 0 \n",
    "print(\"text labeled {}\\n\\n\".format(labels_train[ind]), text_train[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text labeled 1\n",
      "\n",
      " four stories written by robert bloch about various people who live in a beautiful old mansion and what happens to them the first has denholm elliott as a novelist who sees the killer he s writing about come to life some spooky moments and the twist at the end was good the second has peter cushing becoming obsessed with a wax figure resembling his dead wife the third has christopher lee who has a child chloe franks and is scared of her it all leads up to a pretty scary ending although the ending in the story was much worse the last is an out and out comedy with jon petwee and ingrid pitt both chewing the scenery and a cape that turns people into vampires there s also a cute line about christopher lee playing dracula this is a good horror anthology nothing terrifying but the first one and the ending of the third gave me a few pleasurable little chills also the fourth one is actually very funny and pitt makes a very sexy vampire also the house itself looks beautiful and very creepy it s well directed with some nice atmospheric touches a very good and unusual movie score too all in all a good little horror anthology well worth seeking out try to see it on dvd the lions gate one looks fantastic with strong colors and great sound\n"
     ]
    }
   ],
   "source": [
    "ind = 1 \n",
    "print(\"text labeled {}\\n\\n\".format(labels_train[ind]), text_train[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this it looks like the label $y=1$ corresponds to a positive review and $y=0$ corresponds to a negative review. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, the first thing we'll do is train a logistic regression classifier using the Bag-of-Words model, and see what kind of accuracy we can get.  To get started, we need to vectorize the text into numerical features that we can use.  We'll use CountVectorizer to turn our text into features, Scikit-Learn's [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier to do classification, and a pipeline to tie everything together. First though, we'll fit our CountVectorizer independently so we can learn more about the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bagofwords = CountVectorizer()\n",
    "Xbw_train = bagofwords.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: How many different terms are in the vocabulary? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17948 terms in the vocabular\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {:d} terms in the vocabular\".format(Xbw_train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: What are the most common words in the corpus? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word         freq\n",
      "-----------------\n",
      "the          13739\n",
      "and          6568\n",
      "of           5918\n",
      "to           5555\n",
      "is           4351\n",
      "it           3996\n",
      "in           3863\n",
      "this         3037\n",
      "that         2995\n",
      "was          1912\n",
      "as           1853\n",
      "for          1846\n",
      "with         1820\n",
      "movie        1787\n",
      "but          1692\n",
      "film         1621\n",
      "you          1433\n",
      "on           1385\n",
      "not          1239\n",
      "he           1227\n"
     ]
    }
   ],
   "source": [
    "def most_common_words(X, vocab, num_common=20):\n",
    "    word_freqs = X.sum(axis=0)\n",
    "    sorted_indices = np.argsort(word_freqs)[0,::-1]\n",
    "    print(\"word         freq\")\n",
    "    print(\"-----------------\")\n",
    "    for ii in range(num_common):\n",
    "        word_ind = sorted_indices[0,ii]\n",
    "        word =list(vocab.keys())[list(vocab.values()).index(word_ind)] \n",
    "        word_freq = word_freqs[0,word_ind]\n",
    "        print(\"{:12s} {:d}\".format(word, word_freq))\n",
    "\n",
    "most_common_words(Xbw_train, bagofwords.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have some overly common stop words that are probably not useful for classification.  We can remove them by passing in the `stop_words` parameter to the `CountVectorizer`.  We need to tell it which language we're using. How many words are left after removing the stop words? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Next we'll build a pipeline that turns text into features using CountVectorizer and then performs classification using LogisticRegression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "bagofwords_steps = [\n",
    "    (\"bagofwords\", CountVectorizer(stop_words=\"english\")),\n",
    "    (\"logreg\", LogisticRegression())\n",
    "]\n",
    "bagofwords_pipe = Pipeline(bagofwords_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit our model to the training data using the `.fit()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('bagofwords', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "  ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagofwords_pipe.fit(text_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make some predictions.  The following code will print a message form the validation set, along with it's true label, and then make a prediction using the pipeline. Test it out for a few validation examples by changing the value of `ind`.  How does the classifier seem to be doing? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text labeled 0:\n",
      "\n",
      "if you enjoy the original snl cast and shows then avoid this movie at all costs when this first came out my friends and i waited in line for over an hour to get in to a sold out movie house half way through the movie the theatre was empty we refused to leave thinking it would get better when the movie ended we were the only ones left in the theatre the movie lasted only one day in all theaters then vanished from sight in interviews with mr mike he refused to comment on this film the film was an inside joke on the episodes of snl that came out right after the films release and closing in one day we all tried to contact mr mike by phone and mail to get a refund but were totally ignored\n",
      "\n",
      "predicted label: 0\n"
     ]
    }
   ],
   "source": [
    "ind = 1 \n",
    "print(\"text labeled {:d}:\\n\".format(labels_valid[ind]))\n",
    "print(text_valid[ind])\n",
    "pred = bagofwords_pipe.predict([text_valid[ind]])\n",
    "print(\"\\npredicted label: {:d}\".format(pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Finally, let's make predictions on the the entirety of both the training and the validation sets and compute overall accuracies.  We could do this by hand, but LogisticRegression (like most models in Scikit-Learn) includes a `.score()` method which takes in data and the true labels, makes predictions, and then computes the accuracy.  Since LogisticRegression is part of our pipeline, our pipeline inherits this score function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Set:   1.000\n",
      "Accuracy on Validation Set: 0.787\n"
     ]
    }
   ],
   "source": [
    "train_acc = bagofwords_pipe.score(text_train, labels_train)\n",
    "valid_acc = bagofwords_pipe.score(text_valid, labels_valid)\n",
    "print(\"Accuracy on Training Set:   {:.3f}\".format(train_acc))\n",
    "print(\"Accuracy on Validation Set: {:.3f}\".format(valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we're getting $100\\%$ accuracy on the training set (which should worry you) and around a $79\\%$ on the validation set.  Clearly, our features are overfitting the training data.  Let's see if regularization would be helpful.  The `LogisticRegression` method takes a parameter `C` for regularization strength.  The default setting is `C=1`. Try increasing this value and see if the validation accuracy improves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Term Frequency - Inverse Document Frequency \n",
    "***\n",
    "\n",
    "The Bag-of-Words model for text classification is very popular, but let's see if we can do better.  Currently we're weighting every word in the corpus by it's frequency.  It turns out that in text classification there are often features that are not particularly useful predictors for the document class, either because they are too common or too uncommon.  **Stop-words** are extremely common, low-information words like \"a\", \"the\", \"as\", etc.  Removing these from documents is typically the first thing done in peparing data for document classification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Other words that tend to be uninformative predictors are words that appear very very rarely.  In particular, if they do not appear frequently enough in the training data then it is difficult for a classification algorithm to weight them heavily in the classification process. \n",
    "\n",
    "In general, the words that tend to be useful predictors are the words that appear frequently, but not too frequently.  Consider the following frequency graph for a corpus. \n",
    "\n",
    "<img src=\"figs/feat_freq.png\" width=400 height=50>\n",
    "\n",
    "The features in column A appear too frequently to be very useful, and the features in column C appear too rarely. One first-pass method of feature selection in text classification would be to discard the words from columns A and C, and build a classifier with only features from column B.\n",
    "\n",
    "Another common model for identifying the useful terms in a document is the Term Frequency - Inverse Document Frequency (tf-idf) model.  Here we won't throw away any terms, but we'll replace their Bag-of-Words frequency counts with tf-idf scores which we describe below. \n",
    "\n",
    "The tf-idf score is the product of two statistics, *term frequency* and *inverse document frequency*\n",
    "\n",
    "\n",
    "$$\\texttt{tfidf(d,t)} = \\texttt{tf(d,t)} \\times \\texttt{idf(t)}$$\n",
    "\n",
    "The term frequency $\\texttt{tf(d,t)}$ is a measure of the frequency with which term $t$ appears in document $d$.  The inverse document frequency $\\texttt{idf(t)}$ is a measure of how much information the word provides, that is, whether the term is common or rare across all documents.  By multiplying the two quantities together, we obtain a representation of term $t$ in document $d$ that weighs how common the term is in the document with how common the word is in the entire corpus. You can imagine that the words that get the highest associated values are terms that appear many times in a small number of documents. \n",
    "\n",
    "\n",
    "There are many ways to compute the composite terms $\\texttt{tf}$ and $\\texttt{idf}$.  For simplicity, we'll define $\\texttt{tf(d,t)}$ to be the number of times term $t$ appears in document $d$ (i.e., Bag-of-Words). We will define the inverse document frequency as follows: \n",
    "\n",
    "$$\n",
    "\\texttt{idf(t)} = \\log \\left(~ \\frac{1 + \\textrm{total # documents}}{1 + \\textrm{# documents with term }t}\\right) + 1\n",
    " = \\log \\left( \\frac{1+n_d}{1+\\texttt{df}(t)} \\right) + 1\n",
    "$$\n",
    "\n",
    "\n",
    "**Part A**: Compute $\\texttt{idf(t)}$ for the words `tribune` and `new` in our toy training set. \n",
    "\n",
    "$\\texttt{Training Set}:$\n",
    "\n",
    "$\\texttt{d1}: \\texttt{new york new tribune}$\n",
    "\n",
    "$\\texttt{d2}: \\texttt{new york times}$\n",
    "\n",
    "$\\texttt{d3}: \\texttt{los angeles times}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three documents in the training set, so we set $n_d = 3$.  We need to compute $\\texttt{df}(t)$ for $t=$ `tribune` and $t=$ `new`. We have  \n",
    "\n",
    "$$\n",
    "\\texttt{df(tribune)} = 1 \\quad \\textrm{and} \\quad\n",
    "\\texttt{df(new)} = 2 \n",
    "$$\n",
    "\n",
    "Plugging these values into our formula gives \n",
    "\n",
    "$$\n",
    "\\texttt{idf(tribune)} = \\log\\left(\\frac{1+3}{1+1} \\right) + 1 = \\log\\left(2 \\right) + 1 \\approx 1.69 \n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\texttt{idf(new)} = \\log\\left(\\frac{1+3}{1+2} \\right) + 1 = \\log\\left(\\frac{4}{3} \\right) + 1 \\approx 1.28 \n",
    "$$\n",
    "\n",
    "Notice that the `idf` score for tribune is higher than the `idf` score for new, because `tribune` appears in fewer documents than the word `new`.  This means that when we compute `tfidf` features, the word `tribune` gets an extra bump over the word `new`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Compute the `tfidf` feature values for the word `new` in the first two documents, as well as the word `tribune` in the first document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{eqnarray}\n",
    "\\nonumber \\texttt{tfidf(d1, new) = tf(d1,new)} \\times \\texttt{idf(new)} &=& 2 \\times 1.28 = 2.56 \\\\\n",
    "\\nonumber \\texttt{tfidf(d2, new) = tf(d2,new)} \\times \\texttt{idf(new)} &=& 1 \\times 1.28 = 1.28 \\\\\n",
    "\\nonumber \\texttt{tfidf(d1, tribune) = tf(d1,tribune)} \\times \\texttt{idf(tribune)} &=& 1 \\times 1.69 = 1.69 \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "Note that if we compare the feature values for `new` across documents, we see that `new` is given twice the weight in `d1` compared to `d2`, which makes sense because `new` appears twice as frequently in `d1`.  However, despite the fact that `new` appears twice as frequently in `d1` as `tribune`, it does not have twice as much weight due to the `idf` score for `tribune` giving it that extra bump. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: OK, let's construct TFIDF features for our movie reviews, and see if we get a better LogisticRegression classifier.  All we need to do is replace `CountVectorizer` with [`TfidfVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) in our pipeline. If you go to the documentation, you'll notice that `TfidfVectorizer` includes many of the same parameters as `CountVectorizer`, including the ability to automatically remove stop words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Set:   0.993\n",
      "Accuracy on Validation Set: 0.830\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer   \n",
    "\n",
    "# Build the pipeline \n",
    "tfidf_steps = [\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=\"english\")),\n",
    "    (\"logreg\", LogisticRegression(C=1))\n",
    "]\n",
    "tfidf_pipe = Pipeline(tfidf_steps)\n",
    "\n",
    "# Fit model \n",
    "tfidf_pipe.fit(text_train, labels_train)\n",
    "\n",
    "# Get accuracies on training and validation set \n",
    "train_acc = tfidf_pipe.score(text_train, labels_train)\n",
    "valid_acc = tfidf_pipe.score(text_valid, labels_valid)\n",
    "print(\"Accuracy on Training Set:   {:.3f}\".format(train_acc))\n",
    "print(\"Accuracy on Validation Set: {:.3f}\".format(valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: How'd you do?  Did the TFIDF features perform better on the validation set than regular old Bag-of-Words?  What happened with the training set? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we get about a $4\\%$ bump in validation accuracy by using the TFIDF features.  Additionally, the accuracy on the training set went down, indicating that we're not overfitting as much with the TFIDF features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
