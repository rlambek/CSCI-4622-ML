{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Stochastic Gradient Descent\n",
    "***\n",
    "\n",
    "In this notebook we'll implement a rudimentary Stochastic Gradient Descent algorithm to learn the weights in simple linear regression.  Then we'll see if we can make it more efficient.  Finally, we'll investigate some graphical strategies for diagnosing convergence and tuning parameters. \n",
    "\n",
    "**Important Note**: We're basically going to implement 25% of your next homework assignment.  As such, I won't be posting the solutions to this notebook.  Pay attention and follow along. \n",
    "\n",
    "**Semi-Important Note**: There are some helper functions at the bottom of this notebook.  Scroll down and evaluate those before proceeding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pylab as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Setting Up Simulated Data and a Sanity Check \n",
    "***\n",
    "\n",
    "We'll work with simulated data for this exercise where our generative model is given by \n",
    "\n",
    "$$\n",
    "Y = 1 + 2X + \\epsilon \\textrm{ where} \\epsilon \\sim N(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "**Part A**: The following function will generate data from the model. We'll grab a training set of size $n=100$ and a validation set of size $n = 50$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataGenerator(n, sigsq=1.0, random_state=1236):\n",
    "    np.random.seed(random_state)\n",
    "    x_train = np.linspace(-1,1,n)\n",
    "    x_valid = np.linspace(-1,1,int(n/4))\n",
    "    y_train = 1 + 2*x_train + np.random.randn(n)\n",
    "    y_valid = 1 + 2*x_valid + np.random.randn(int(n/4))\n",
    "    return x_train, x_valid, y_train, y_valid \n",
    "\n",
    "x_train, x_valid, y_train, y_valid = dataGenerator(100)\n",
    "    \n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "ax.scatter(x_train, y_train, color=\"steelblue\", s=100, label=\"train\")\n",
    "ax.scatter(x_valid, y_valid, color=\"#a76c6e\", s=100, label=\"valid\")\n",
    "ax.grid(alpha=0.25)\n",
    "ax.set_axisbelow(True)\n",
    "ax.set_xlabel(\"X\", fontsize=16)\n",
    "ax.set_ylabel(\"Y\", fontsize=16)\n",
    "ax.legend(loc=\"upper left\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Since we're going to be implementing things ourselves, we're going to want to prepend the data matrices with a column of ones so we can fit a bias term.  We can do this using numpy's [column_stack](https://docs.scipy.org/doc/numpy/reference/generated/numpy.column_stack.html) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.column_stack((np.ones_like(x_train), x_train))\n",
    "X_valid = np.column_stack((np.ones_like(x_valid), x_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Finally, let's fit a linear regression model with sklearn's LinearRegression class and print the coefficients so we know what we're shooting for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "reg = LinearRegression(fit_intercept=False)\n",
    "reg.fit(X_train, y_train)\n",
    "print(\"sklearn says the coefficients are \", reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: The last thing we'll do is visualize the surface of the RSS, of which we're attempting to find the minimum. Does it looks like the parameters reported by sklearn lie at the bottom of the RSS surface?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotsurface(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Implementing and Improving SGD \n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Now it's time to implement Stochastic Gradient Descent.  Most of the code in the function sgd has been written for you.  Your job is to fill in the values of the partial derivatives in the appropriate places.  Recall that the update scheme is given by \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\beta_0 &\\leftarrow& \\beta_0 - \\eta \\cdot 2 \\cdot \\left[(\\beta_0 + \\beta_1x_i) -y_i \\right] \\\\\n",
    "\\beta_1 &\\leftarrow& \\beta_1 - \\eta \\cdot 2 \\cdot \\left[(\\beta_0 + \\beta_1x_i) -y_i \\right] x_i\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Note that the function parameter beta is a numpy array containing the initial guess for the solve. The numpy array bhist stores the approximation of the betas after each iteration for plotting and diagnostic purposes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(X, y, beta, eta=0.1, num_epochs=100):\n",
    "    \"\"\"\n",
    "    Peform Stochastic Gradient Descent \n",
    "    \n",
    "    :param X: matrix of training features \n",
    "    :param y: vector of training responses \n",
    "    :param beta: initial guess for the parameters\n",
    "    :param eta: the learning rate \n",
    "    :param num_epochs: the number of epochs to run \n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize history for plotting \n",
    "    bhist = np.zeros((num_epochs+1, len(beta)))\n",
    "    bhist[0,0], bhist[0,1] = beta[0], beta[1]\n",
    "    \n",
    "    # perform epochs \n",
    "    for ee in range(1,num_epochs+1):\n",
    "        \n",
    "        # shuffle indices \n",
    "        shuffled_inds = list(range(X.shape[0]))\n",
    "        np.random.shuffle(shuffled_inds)\n",
    "        \n",
    "        # loop over training examples \n",
    "        for ii in shuffled_inds:\n",
    "            beta[0] = beta[0] - 0.0 # TODO \n",
    "            beta[1] = beta[1] - 0.0 # TODO \n",
    "\n",
    "        # save history \n",
    "        bhist[ee,:] = beta\n",
    "        \n",
    "    # return bhist. Last row \n",
    "    # are the learned parameters. \n",
    "    return bhist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start at (-2,1)\n",
    "beta_start = np.array([-2.0, -1.0])\n",
    "\n",
    "# Training \n",
    "%time bhist = sgd(X_train, y_train, beta=beta_start, eta=0.0025, num_epochs=1000) # old = 0.0025\n",
    "\n",
    "# Print and Plot \n",
    "print(\"beta_0 = {:.5f}, beta_1 = {:.5f}\".format(bhist[-1][0], bhist[-1][1]))\n",
    "plotsurface(X_train, y_train, bhist=bhist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Does anything in this computation seem redundant to you?  Think about the case when we have more than two features. When you see it, go back to the `sgd` function and improve it.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Again, thinking about the case where we have more than two features, can you think of a way to vectorize the stochastic gradient update of the parameters? When you see it, go back to the sgd function and improve it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Now that you have created this beautiful solver, go back and break it by playing with the learning rate. Does the learning rate have the effect on convergence that you expect when visualized in the surface plot? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Graphical Diagnosis of Convergence \n",
    "***\n",
    "\n",
    "A common way to monitor the convergence of SGD and to tune hyperparameters (like learning rate and regularization strength) is to make a plot of how the loss function evolves during the training process. That is, we plot the value of the loss function periodically and see if it looks like it's reached a minimum, or see if it's jumping around a lot.  Normally we'd record the value of the loss function as we train, but we'll use the beta histories returned by our solver.  Finally, using the MSE instead of the RSS is a popular choice, so we'll do that.  \n",
    "\n",
    "**Part A**: Modify the function below to take in a beta history and a data set and return a vector of MSE values for each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MSE_hist(X, y, bhist):\n",
    "    mse = np.zeros(bhist.shape[0])\n",
    "    for ii in range(bhist.shape[0]):\n",
    "        mse[ii] = 0\n",
    "    return mse \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Next we'll take the MSE history that we just computed and plot it vs epoch number. Based on your plot, would you say that your MSE has converged? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = MSE_hist(X_train, y_train, bhist)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "ax.plot(range(1,len(mse_train)+1), mse_train, color=\"steelblue\", label=\"train\")\n",
    "ax.set_xlabel(\"epoch number\", fontsize=16)\n",
    "ax.set_ylabel(\"mse\", fontsize=16);\n",
    "ax.grid(alpha=0.25)\n",
    "ax.legend(loc=\"upper right\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Go back up and change the value of the learning rate to bigger and smaller values (you might also have to adjust the max epochs).  Do the different learning rates have the effect on the MSE plots that you would expect? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: OK, now for the real question.  Is the MSE on the training data the best thing to look at when deciding if our training algorithm has converged? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: The Learning Rate Schedule Game\n",
    "***\n",
    "\n",
    "In the case when your negative log-likelihood function is convex, the choice of learning rate mainly affects the convergence of your SGD routine.  In a nonconvex problem, the choice of learning rate can determine whether you find the global minimum, or get stuck forever in a local minimum.  In most sophisticated optimization routines, the learning rate is adapted over time.  Varying learning rate schedules allow you to explore local minimums but still be able to make it out and eventually find the global minimum.  \n",
    "\n",
    "The following game is a cheap facsimile of stochastic gradient descent.  There is no negative log-likelihood function, or training set.  You just have a simple function that you would like to minimize, namely \n",
    "\n",
    "$$\n",
    "f(x,y) = 5-\\sin(3 \\pi x) ~ \\sin(3 \\pi y) - 3~\\textrm{exp}\\left[{-\\left(x-\\frac{1}{2}\\right)^2 - \\left(y-\\frac{1}{2}\\right)^2}\\right]\n",
    "$$\n",
    "\n",
    "The surface looks as follows.  Notice that there is a global minimum at $(1/2,1/2)$ and several local maxima and minima surrounding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nc_surface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've given you a starting point and a basic gradient descent algorithm (located in the Helper Functions section below).  Below this cell there is a learning rate scheduling function that currently just returns the initial learning rate that you prescribe.  The goal of this game is for you to adjust the initial learning rate and the scheduling function that allows the iterate to make it to the global minimum.  The **only** things you're allowed to change are the **initial learning rate** and the **schedule function**.   Before you can play you need to evaluate the code-blocks at the bottom of the page.  Then come back and evaluate the $\\texttt{playgame}$ function with it's current inputs and see what happens! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule(k, n, eta0):\n",
    "    '''\n",
    "    :param k: The current iteration \n",
    "    :param n: The max number of iterations\n",
    "    :param eta0: The original learning rate \n",
    "    '''\n",
    "    return eta0\n",
    "\n",
    "playgame(np.array([0.15,0.0]), 150, .005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Hint**: If you're not having much luck, try implementing a schedule of the form \n",
    "$\n",
    "\\eta_k = \\dfrac{\\eta_0}{ 1 + \\alpha ~ k~/~n}\n",
    "$ where here $\\alpha$ is a tuning parameter.  You'll probably also have to make your initial learning rate bigger. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mycolors = {\"blue\":\"steelblue\", \"red\":\"#a76c6e\",  \"green\":\"#6a9373\", \"smoke\":\"#f2f2f2\"}\n",
    "\n",
    "def eval_RSS(X, y, b0, b1):\n",
    "    rss = 0 \n",
    "    for ii in range(len(df)):\n",
    "        xi = df.loc[ii,\"x\"]\n",
    "        yi = df.loc[ii,\"y\"]\n",
    "        rss += (yi - (b0 + b1*xi))**2\n",
    "    return rss\n",
    "\n",
    "def plotsurface(X, y, bhist=None):\n",
    "    xx, yy = np.meshgrid(np.linspace(-3, 3, 300), np.linspace(-1, 5, 300))\n",
    "    Z = np.zeros((xx.shape[0], yy.shape[0]))\n",
    "    for ii in range(X.shape[0]):\n",
    "        Z += (y[ii] - xx - yy * X[ii,1])**2\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,10))\n",
    "    levels = [125, 200] + list(range(400,2000,400))\n",
    "    CS = ax.contour(xx, yy, Z, levels=levels)\n",
    "    ax.clabel(CS, CS.levels, inline=True, fontsize=10)\n",
    "    ax.set_xlim([-3,3])\n",
    "    ax.set_ylim([-1,5])\n",
    "    ax.set_xlabel(r\"$\\beta_0$\", fontsize=20)\n",
    "    ax.set_ylabel(r\"$\\beta_1$\", fontsize=20)\n",
    "    if bhist is not None:\n",
    "        for ii in range(bhist.shape[0]-1):\n",
    "            x0 = bhist[ii][0]\n",
    "            y0 = bhist[ii][1]\n",
    "            x1 = bhist[ii+1][0]\n",
    "            y1 = bhist[ii+1][1]\n",
    "            ax.plot([x0, x1], [y0,y1], color=\"black\", marker=\"o\", lw=1.5, markersize=5)\n",
    "            \n",
    "def myncfun(x, y): \n",
    "    return 5 - np.sin(3*np.pi*x)*np.sin(3*np.pi*y) - 3*np.exp(-(x-.5)**2 - (y-.5)**2)\n",
    "\n",
    "def myncgrad(x):\n",
    "    g1 = -3 * np.pi * np.cos(3*np.pi*x[0]) * np.sin(3*np.pi*x[1]) + 6 * (x[0]-.5) * np.exp(-(x[0]-.5)**2 - (x[1]-.5)**2)\n",
    "    g2 = -3 * np.pi * np.sin(3*np.pi*x[0]) * np.cos(3*np.pi*x[1]) + 6 * (x[1]-.5) * np.exp(-(x[0]-.5)**2 - (x[1]-.5)**2)\n",
    "    return np.array([g1, g2])\n",
    "\n",
    "def detSGD(x, numstep, eta0):\n",
    "    '''\n",
    "    :param x: Starting point \n",
    "    :param numstep: Total number iterations \n",
    "    :param eta0: Initial learning rate \n",
    "    '''\n",
    "    xhist = np.zeros((numstep+1,2))\n",
    "    xhist[0,:] = x \n",
    "    for kk in range(numstep):\n",
    "        x = x - schedule(kk, numstep, eta0) * myncgrad(x)\n",
    "        xhist[kk+1,:] = x \n",
    "    return xhist\n",
    "\n",
    "def playgame(x0, numstep, eta0):\n",
    "    '''\n",
    "    :param x0: The starting point \n",
    "    :param numstep: The total number of iterations to do \n",
    "    :param eta0: The original learning rate \n",
    "    '''\n",
    "    xx, yy = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 200))\n",
    "    Z = myncfun(xx, yy)\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    CS = plt.contour(xx, yy, Z)\n",
    "    plt.clabel(CS, inline=1, fontsize=10)\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1])\n",
    "\n",
    "    xhist = detSGD(x0, numstep, eta0)\n",
    "    \n",
    "    fvals = np.zeros(numstep+1)\n",
    "    fvals[0] = myncfun(x0[0], x0[1])\n",
    "\n",
    "    for ii in range(xhist.shape[0]-1):\n",
    "        x0 = xhist[ii][0]\n",
    "        y0 = xhist[ii][1]\n",
    "        x1 = xhist[ii+1][0]\n",
    "        y1 = xhist[ii+1][1]\n",
    "        ax1.plot([x0, x1], [y0,y1], color=\"black\", marker=\"o\", lw=1.5, markersize=5)\n",
    "        fvals[ii+1] = myncfun(x0, y0)\n",
    "        \n",
    "    plt.xlabel(\"x1\", fontsize=16)\n",
    "    plt.ylabel(\"x2\", fontsize=16)\n",
    "        \n",
    "    maxval = myncfun(0.5,0.5)\n",
    "        \n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.plot(fvals, 'r--', marker=\"o\")\n",
    "    ax2.plot([0, numstep+1], [maxval, maxval], 'k--', lw=2, alpha=0.5)\n",
    "    plt.xlim([0,numstep+1])\n",
    "    plt.ylim([.5,3])\n",
    "    plt.xlabel(\"iteration\", fontsize=16)\n",
    "    plt.ylabel(\"function value\", fontsize=16);\n",
    "    \n",
    "def plot_nc_surface():\n",
    "    xx, yy = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 200))\n",
    "    Z = myncfun(xx, yy)\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    CS = plt.contour(xx, yy, Z)\n",
    "    plt.clabel(CS, inline=1, fontsize=10)\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1])\n",
    "    plt.xlabel(\"x1\", fontsize=16)\n",
    "    plt.ylabel(\"x2\", fontsize=16)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
